{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzFG0X778vd8"
      },
      "source": [
        "# Análisis de Wavelets para Datos MI-EEG\n",
        "\n",
        "Este notebook implementa análisis avanzado de wavelets para extraer características temporales y espectrales de los datos EEG de imaginación motora.\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Transformada Wavelet Continua (CWT)**: Análisis tiempo-frecuencia usando wavelet de Morlet\n",
        "2. **Transformada Wavelet Discreta (DWT)**: Descomposición multiresolución usando Daubechies 4\n",
        "3. **Extracción de características**: Energía por banda, frecuencia dominante, entropía espectral\n",
        "4. **Preparación para BoF**: Características optimizadas para Bag of Features\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "- Carga y procesamiento de datos EEG (independiente del EDA)\n",
        "- Análisis CWT con escalas logarítmicas\n",
        "- Análisis DWT con múltiples niveles\n",
        "- Extracción de características estadísticas\n",
        "- Visualizaciones tiempo-frecuencia\n",
        "- Guardado de características para BoF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j17H_zSa8y7j",
        "outputId": "eb069985-669f-4f63-b439-ab101831dd08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_FLdbWv8vd-",
        "outputId": "0ad55e0e-293c-4fc6-dfa5-5a341c67976b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.10.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.10.5)\n",
            "Downloading mne-1.10.2-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.10.2\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from PyWavelets) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Instalación de dependencias completada\n"
          ]
        }
      ],
      "source": [
        "# Celda de instalación de dependencias\n",
        "# Ejecutar esta celda SOLO si necesitas instalar las librerías en un entorno nuevo\n",
        "\n",
        "%pip install mne\n",
        "%pip install PyWavelets\n",
        "%pip install scikit-learn\n",
        "%pip install matplotlib\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install scipy\n",
        "%pip install tqdm\n",
        "%pip install joblib\n",
        "\n",
        "print(\"Instalación de dependencias completada\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIc7Q6GW8vd_",
        "outputId": "97a21d93-a788-4f66-a0e5-bd5a62bbd70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesadores disponibles: 2\n",
            "Cores a utilizar: 1\n",
            "Librerías importadas correctamente\n",
            "PyWavelets versión: 1.8.0\n"
          ]
        }
      ],
      "source": [
        "# Importar librerías necesarias\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pywt\n",
        "from mne.io import read_raw_eeglab\n",
        "from scipy.signal import welch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "# Configurar matplotlib\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Configuración de paralelización\n",
        "N_JOBS = max(1, multiprocessing.cpu_count() - 1)  # Usar todos los cores menos uno\n",
        "print(f\"Procesadores disponibles: {multiprocessing.cpu_count()}\")\n",
        "print(f\"Cores a utilizar: {N_JOBS}\")\n",
        "\n",
        "print(\"Librerías importadas correctamente\")\n",
        "print(f\"PyWavelets versión: {pywt.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lgghW028vd_"
      },
      "source": [
        "## Configuración de Parámetros y Carga de Datos\n",
        "\n",
        "Configuramos los parámetros necesarios y cargamos los datos EEG directamente desde los archivos originales:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa4rS0h48veA"
      },
      "source": [
        "## Carga de Datos EEG\n",
        "\n",
        "Cargamos todos los archivos de datos de imaginación motora (left/right) y los procesamos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q58Ew2dX8veA",
        "outputId": "cd9cf9c0-1d44-40e3-cf51-7d45a14aa709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parámetros y funciones auxiliares definidas\n",
            "  - Bandas de frecuencia: 8.0-30.0 Hz\n",
            "  - Banda μ: 10.0-12.0 Hz\n",
            "  - Banda β: 18.0-26.0 Hz\n",
            "  - Duración por trial: 9.0 segundos\n"
          ]
        }
      ],
      "source": [
        "# Parámetros de análisis\n",
        "LOW_BAND, HIGH_BAND = 8.0, 30.0  # Rango de frecuencias de interés\n",
        "MU_BAND = (10.0, 12.0)           # Banda mu (ritmo sensoriomotor)\n",
        "BETA_BAND = (18.0, 26.0)         # Banda beta\n",
        "EXPECTED_TRIAL_SEC = 9.0         # Duración esperada de cada trial\n",
        "\n",
        "# Directorios de datos\n",
        "CANDIDATE_DIRS = {\n",
        "    \"left_imag\":  (\"left\",  \"imag\"),\n",
        "    \"right_imag\": (\"right\", \"imag\"),\n",
        "}\n",
        "\n",
        "# Regiones cerebrales por prefijos del sistema 10-20\n",
        "REGION_PREFIXES = {\n",
        "    \"F\" : (\"Fp\", \"AF\", \"F\"),    # Frontal\n",
        "    \"FC\": (\"FC\",),              # Frontocentral\n",
        "    \"C\" : (\"C\", \"Cz\"),          # Central\n",
        "    \"CP\": (\"CP\",),              # Centroparietal\n",
        "    \"P\" : (\"P\",),               # Parietal\n",
        "    \"PO\": (\"PO\",),              # Parietooccipital\n",
        "    \"O\" : (\"O\",),               # Occipital\n",
        "}\n",
        "\n",
        "# Funciones auxiliares para carga de datos\n",
        "def subject_from_fname(fname: str) -> str:\n",
        "    \"\"\"Extrae el ID del sujeto del nombre del archivo.\"\"\"\n",
        "    m = re.search(r\"(S\\d{3})\", os.path.basename(fname))\n",
        "    return m.group(1) if m else os.path.basename(fname)\n",
        "\n",
        "def try_read_epochs(fname: str) -> mne.BaseEpochs:\n",
        "    \"\"\"Lee epochs de EEGLAB, creando epochs si es necesario.\"\"\"\n",
        "    # 1) Si ya viene epocado\n",
        "    try:\n",
        "        ep = mne.read_epochs_eeglab(fname, verbose=\"ERROR\")\n",
        "        _ = ep.get_data()\n",
        "        return ep\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Continuo -> ventaneo simple de 9s\n",
        "    raw = read_raw_eeglab(fname, preload=True, verbose=\"ERROR\")\n",
        "    try:\n",
        "        raw.filter(LOW_BAND, HIGH_BAND, verbose=\"ERROR\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    sfreq = float(raw.info[\"sfreq\"])\n",
        "    n_win = int(np.floor(raw.times[-1] / EXPECTED_TRIAL_SEC))\n",
        "    if n_win < 1:\n",
        "        data = np.expand_dims(raw.get_data(), axis=0)\n",
        "        return mne.EpochsArray(data, raw.info, tmin=0.0, verbose=\"ERROR\")\n",
        "\n",
        "    picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
        "    samps = int(EXPECTED_TRIAL_SEC * sfreq)\n",
        "    data_list: List[np.ndarray] = []\n",
        "\n",
        "    for i in range(n_win):\n",
        "        s, e = i * samps, (i + 1) * samps\n",
        "        if e <= raw.n_times:\n",
        "            data_list.append(np.expand_dims(raw.get_data(picks=picks)[:, s:e], axis=0))\n",
        "\n",
        "    data = np.concatenate(data_list, axis=0)\n",
        "    info = mne.create_info([raw.ch_names[p] for p in picks], sfreq, ch_types=\"eeg\")\n",
        "    return mne.EpochsArray(data, info, tmin=0.0, verbose=\"ERROR\")\n",
        "\n",
        "print(\"Parámetros y funciones auxiliares definidas\")\n",
        "print(f\"  - Bandas de frecuencia: {LOW_BAND}-{HIGH_BAND} Hz\")\n",
        "print(f\"  - Banda μ: {MU_BAND[0]}-{MU_BAND[1]} Hz\")\n",
        "print(f\"  - Banda β: {BETA_BAND[0]}-{BETA_BAND[1]} Hz\")\n",
        "print(f\"  - Duración por trial: {EXPECTED_TRIAL_SEC} segundos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIOnRk538veA",
        "outputId": "fbdd71fc-cef9-4735-a353-077e2e7a551e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directorio de datos: /content\n",
            "Cargando datos de imaginación motora...\n",
            "Procesando directorio: drive/MyDrive/eeg/left_imag\n",
            "  S001: 22 trials\n",
            "  S002: 22 trials\n",
            "  S003: 23 trials\n",
            "  S004: 23 trials\n",
            "  S005: 21 trials\n",
            "  S006: 23 trials\n",
            "  S007: 22 trials\n",
            "  S008: 22 trials\n",
            "  S009: 23 trials\n",
            "  S010: 23 trials\n",
            "  S011: 23 trials\n",
            "  S012: 21 trials\n",
            "  S013: 22 trials\n",
            "  S014: 22 trials\n",
            "  S015: 22 trials\n",
            "  S016: 21 trials\n",
            "  S017: 22 trials\n",
            "  S018: 21 trials\n",
            "  S019: 22 trials\n",
            "  S020: 22 trials\n",
            "Procesando directorio: drive/MyDrive/eeg/right_imag\n",
            "  S001: 22 trials\n",
            "  S002: 22 trials\n",
            "  S003: 21 trials\n",
            "  S004: 21 trials\n",
            "  S005: 23 trials\n",
            "  S006: 21 trials\n",
            "  S007: 22 trials\n",
            "  S008: 22 trials\n",
            "  S009: 21 trials\n",
            "  S010: 21 trials\n",
            "  S011: 21 trials\n",
            "  S012: 23 trials\n",
            "  S013: 22 trials\n",
            "  S014: 22 trials\n",
            "  S015: 22 trials\n",
            "  S016: 23 trials\n",
            "  S017: 22 trials\n",
            "  S018: 23 trials\n",
            "  S019: 22 trials\n",
            "  S020: 22 trials\n",
            "\n",
            "Resumen de carga:\n",
            "  - Archivos procesados: 20\n",
            "  - Total de epochs: 40\n",
            "Datos cargados correctamente\n"
          ]
        }
      ],
      "source": [
        "# Cargar epochs de imaginación (left/right)\n",
        "epochs_list: List[mne.BaseEpochs] = []\n",
        "subjects_info = []\n",
        "\n",
        "# Configuración de directorios\n",
        "data_root = Path(\".\").resolve()  # Directorio actual\n",
        "print(f\"Directorio de datos: {data_root}\")\n",
        "\n",
        "print(\"Cargando datos de imaginación motora...\")\n",
        "\n",
        "for dirname in [\"drive/MyDrive/eeg/left_imag\", \"drive/MyDrive/eeg/right_imag\"]:\n",
        "    dpath = data_root / dirname\n",
        "    if not dpath.is_dir():\n",
        "        print(f\"Advertencia: No existe {dpath}, omitiendo...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Procesando directorio: {dirname}\")\n",
        "    files_processed = 0\n",
        "\n",
        "    for set_path in sorted(glob(str(dpath / \"*.set\"))):\n",
        "        try:\n",
        "            subject_id = subject_from_fname(set_path)\n",
        "            ep = try_read_epochs(set_path)\n",
        "            epochs_list.append(ep)\n",
        "            subjects_info.append({\n",
        "                'subject': subject_id,\n",
        "                'task': dirname.split('_')[0],  # 'left' o 'right'\n",
        "                'file': str(set_path),\n",
        "                'n_trials': ep.get_data().shape[0]\n",
        "            })\n",
        "            files_processed += 1\n",
        "            print(f\"  {subject_id}: {ep.get_data().shape[0]} trials\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error en {set_path}: {e}\")\n",
        "\n",
        "print(f\"\\nResumen de carga:\")\n",
        "print(f\"  - Archivos procesados: {files_processed}\")\n",
        "print(f\"  - Total de epochs: {len(epochs_list)}\")\n",
        "\n",
        "if not epochs_list:\n",
        "    raise ValueError(\"Error: No se cargaron epochs. Verificar archivos de datos.\")\n",
        "else:\n",
        "    print(\"Datos cargados correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTb613ih8veB",
        "outputId": "8c9c79a0-515c-45ba-c1f3-75cc8be118fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuración de directorios:\n",
            "  - Directorio de datos: /content\n",
            "  - Directorio de salida EDA: /content/reports\n",
            "  - Directorio de salida wavelets: /content/wavelet_reports\n"
          ]
        }
      ],
      "source": [
        "# Configuración de directorios\n",
        "data_root = Path(\".\").resolve()  # Directorio actual\n",
        "output_dir = Path(\"reports\")\n",
        "wavelet_output_dir = Path(\"wavelet_reports\")\n",
        "wavelet_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nConfiguración de directorios:\")\n",
        "print(f\"  - Directorio de datos: {data_root}\")\n",
        "print(f\"  - Directorio de salida EDA: {output_dir.resolve()}\")\n",
        "print(f\"  - Directorio de salida wavelets: {wavelet_output_dir.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w1HlUBX8veB"
      },
      "source": [
        "## Carga de Datos EEG\n",
        "\n",
        "Cargamos todos los archivos de datos de imaginación motora (left/right) y los procesamos:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WQHNqz48veB"
      },
      "source": [
        "## Preparación de Datos\n",
        "\n",
        "Concatenamos todos los epochs y preparamos los datos para el análisis de wavelets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eiZ4cDA8veB",
        "outputId": "ecf63c61-5cfe-4ade-85b6-56fa1988091f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Información de los datos:\n",
            "  - Canales: 64\n",
            "  - Frecuencia de muestreo: 128.0 Hz\n",
            "  - Primeros 10 canales: ['FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'C5', 'C3', 'C1']\n",
            "\n",
            "Datos concatenados:\n",
            "  - Trials totales: 880\n",
            "  - Canales: 64\n",
            "  - Muestras por trial: 1152\n",
            "  - Duración por trial: 9.0 segundos\n",
            "\n",
            "Regiones identificadas: ['F', 'FC', 'C', 'CP', 'P', 'PO', 'O']\n",
            "  - Región F: 26 canales\n",
            "  - Región FC: 7 canales\n",
            "  - Región C: 14 canales\n",
            "  - Región CP: 7 canales\n",
            "  - Región P: 14 canales\n",
            "  - Región PO: 5 canales\n",
            "  - Región O: 3 canales\n"
          ]
        }
      ],
      "source": [
        "# Concatenar epochs y verificar consistencia\n",
        "base_info = epochs_list[0].info\n",
        "ch_names = epochs_list[0].ch_names\n",
        "sfreq = float(epochs_list[0].info[\"sfreq\"])\n",
        "\n",
        "print(f\"\\nInformación de los datos:\")\n",
        "print(f\"  - Canales: {len(ch_names)}\")\n",
        "print(f\"  - Frecuencia de muestreo: {sfreq} Hz\")\n",
        "print(f\"  - Primeros 10 canales: {ch_names[:10]}\")\n",
        "\n",
        "# Verificar consistencia entre archivos\n",
        "for i, ep in enumerate(epochs_list[1:], 1):\n",
        "    if ep.ch_names != ch_names:\n",
        "        print(f\"Advertencia: Los órdenes de canales difieren en archivo {i}\")\n",
        "    if int(ep.info[\"sfreq\"]) != int(sfreq):\n",
        "        print(f\"Advertencia: sfreq inconsistente en archivo {i}\")\n",
        "\n",
        "# Concatenar todos los datos\n",
        "X = np.concatenate([ep.get_data() for ep in epochs_list], axis=0)  # (N, ch, T)\n",
        "N, C, T = X.shape\n",
        "\n",
        "print(f\"\\nDatos concatenados:\")\n",
        "print(f\"  - Trials totales: {N}\")\n",
        "print(f\"  - Canales: {C}\")\n",
        "print(f\"  - Muestras por trial: {T}\")\n",
        "print(f\"  - Duración por trial: {T/sfreq:.1f} segundos\")\n",
        "\n",
        "# Agrupar canales por regiones cerebrales\n",
        "ch_upper = [c.upper() for c in ch_names]\n",
        "region_indices: Dict[str, List[int]] = {}\n",
        "\n",
        "for reg, prefixes in REGION_PREFIXES.items():\n",
        "    idxs = []\n",
        "    for i, c in enumerate(ch_upper):\n",
        "        if any(c.startswith(p) for p in prefixes):\n",
        "            idxs.append(i)\n",
        "    region_indices[reg] = idxs\n",
        "\n",
        "print(f\"\\nRegiones identificadas: {list(region_indices.keys())}\")\n",
        "for reg, idxs in region_indices.items():\n",
        "    if idxs:\n",
        "        print(f\"  - Región {reg}: {len(idxs)} canales\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak483Y158veC"
      },
      "source": [
        "## Guardado Opcional de Datos Procesados\n",
        "\n",
        "Opcionalmente, guardamos los datos procesados para uso posterior o compatibilidad con otros notebooks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKd76y8j8veC",
        "outputId": "8925b856-9396-42ac-d971-15444a66383e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos compartidos guardados en: /content/shared_data\n",
            "Archivos generados:\n",
            "  - X_data.npy: Datos concatenados ((880, 64, 1152))\n",
            "  - ch_names.npy: Nombres de canales (64 canales)\n",
            "  - sfreq.npy: Frecuencia de muestreo (128.0 Hz)\n",
            "  - data_dimensions.npy: Dimensiones (880, 64, 1152)\n",
            "  - subjects_info.csv: Información de sujetos\n",
            "  - region_info.json: Información de regiones\n",
            "  - config_params.json: Parámetros de configuración\n",
            "\n",
            "Datos preparados para análisis de wavelets:\n",
            "  - Datos concatenados: (880, 64, 1152) (trials, channels, time)\n",
            "  - Canales: 64\n",
            "  - Frecuencia de muestreo: 128.0 Hz\n",
            "  - Dimensiones: 880 trials, 64 canales, 1152 muestras\n",
            "  - Duración por trial: 9.0 segundos\n",
            "  - Sujetos procesados: 40\n",
            "  - Regiones identificadas: ['F', 'FC', 'C', 'CP', 'P', 'PO', 'O']\n"
          ]
        }
      ],
      "source": [
        "# Crear directorio para datos compartidos (opcional, para compatibilidad)\n",
        "shared_data_dir = Path(\"shared_data\")\n",
        "shared_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Guardar datos principales\n",
        "np.save(shared_data_dir / \"X_data.npy\", X)  # Datos concatenados (trials, channels, time)\n",
        "np.save(shared_data_dir / \"ch_names.npy\", ch_names)  # Nombres de canales\n",
        "np.save(shared_data_dir / \"sfreq.npy\", np.array([sfreq]))  # Frecuencia de muestreo\n",
        "np.save(shared_data_dir / \"data_dimensions.npy\", np.array([N, C, T]))  # Dimensiones\n",
        "\n",
        "# Guardar información de sujetos\n",
        "subjects_df = pd.DataFrame(subjects_info)\n",
        "subjects_df.to_csv(shared_data_dir / \"subjects_info.csv\", index=False)\n",
        "\n",
        "# Guardar información de regiones\n",
        "region_info = {\n",
        "    'region_indices': region_indices,\n",
        "    'region_prefixes': REGION_PREFIXES\n",
        "}\n",
        "with open(shared_data_dir / \"region_info.json\", 'w') as f:\n",
        "    json.dump(region_info, f, indent=2)\n",
        "\n",
        "# Guardar parámetros de configuración\n",
        "config_params = {\n",
        "    'LOW_BAND': LOW_BAND,\n",
        "    'HIGH_BAND': HIGH_BAND,\n",
        "    'MU_BAND': list(MU_BAND),\n",
        "    'BETA_BAND': list(BETA_BAND),\n",
        "    'EXPECTED_TRIAL_SEC': EXPECTED_TRIAL_SEC,\n",
        "    'CANDIDATE_DIRS': CANDIDATE_DIRS\n",
        "}\n",
        "with open(shared_data_dir / \"config_params.json\", 'w') as f:\n",
        "    json.dump(config_params, f, indent=2)\n",
        "\n",
        "print(f\"Datos compartidos guardados en: {shared_data_dir.resolve()}\")\n",
        "print(\"Archivos generados:\")\n",
        "print(f\"  - X_data.npy: Datos concatenados ({X.shape})\")\n",
        "print(f\"  - ch_names.npy: Nombres de canales ({len(ch_names)} canales)\")\n",
        "print(f\"  - sfreq.npy: Frecuencia de muestreo ({sfreq} Hz)\")\n",
        "print(f\"  - data_dimensions.npy: Dimensiones ({N}, {C}, {T})\")\n",
        "print(f\"  - subjects_info.csv: Información de sujetos\")\n",
        "print(f\"  - region_info.json: Información de regiones\")\n",
        "print(f\"  - config_params.json: Parámetros de configuración\")\n",
        "\n",
        "print(\"\\nDatos preparados para análisis de wavelets:\")\n",
        "print(f\"  - Datos concatenados: {X.shape} (trials, channels, time)\")\n",
        "print(f\"  - Canales: {len(ch_names)}\")\n",
        "print(f\"  - Frecuencia de muestreo: {sfreq} Hz\")\n",
        "print(f\"  - Dimensiones: {N} trials, {C} canales, {T} muestras\")\n",
        "print(f\"  - Duración por trial: {T/sfreq:.1f} segundos\")\n",
        "print(f\"  - Sujetos procesados: {len(subjects_info)}\")\n",
        "print(f\"  - Regiones identificadas: {list(region_indices.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t871sJK8veC"
      },
      "source": [
        "## Configuración de Wavelets\n",
        "\n",
        "Definimos los parámetros para el análisis de wavelets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqFHytAt8veC",
        "outputId": "bec45c85-3b95-4e1d-d787-8617068c30a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuración de wavelets:\n",
            "Escalas CWT: 50 escalas desde 3.2 hasta 316.2\n",
            "Wavelet CWT: cmor5.0-1.0 con ancho 5.0\n",
            "Wavelet DWT: db4 con 6 niveles\n",
            "Bandas de frecuencia: ['delta', 'theta', 'alpha', 'beta', 'gamma']\n"
          ]
        }
      ],
      "source": [
        "# Configurar parámetros de wavelets\n",
        "CWT_SCALES = np.logspace(0.5, 2.5, 50)  # Escalas logarítmicas (1.6-316 Hz aprox)\n",
        "CWT_WAVELET = 'cmor5.0-1.0'  # Complex Morlet wavelet para análisis tiempo-frecuencia (ancho 5.0, frecuencia central 1.0)\n",
        "CWT_WIDTH = 5.0  # Ancho de la wavelet de Morlet\n",
        "\n",
        "# Parámetros DWT\n",
        "DWT_WAVELET = 'db4'  # Wavelet Daubechies 4\n",
        "DWT_LEVELS = 6  # Niveles de descomposición\n",
        "\n",
        "# Bandas de frecuencia de interés para wavelets\n",
        "FREQ_BANDS = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 13),\n",
        "    'beta': (13, 30),\n",
        "    'gamma': (30, 100)\n",
        "}\n",
        "\n",
        "print(\"Configuración de wavelets:\")\n",
        "print(f\"Escalas CWT: {len(CWT_SCALES)} escalas desde {CWT_SCALES[0]:.1f} hasta {CWT_SCALES[-1]:.1f}\")\n",
        "print(f\"Wavelet CWT: {CWT_WAVELET} con ancho {CWT_WIDTH}\")\n",
        "print(f\"Wavelet DWT: {DWT_WAVELET} con {DWT_LEVELS} niveles\")\n",
        "print(f\"Bandas de frecuencia: {list(FREQ_BANDS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohn7ogBI8veD"
      },
      "source": [
        "### Funciones Auxiliares para Wavelets\n",
        "\n",
        "Definimos las funciones específicas para el análisis de wavelets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD6VxGnx8veD",
        "outputId": "1934d9b8-0dd2-4f34-cbec-abf822066d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funciones auxiliares para wavelets definidas (versión optimizada)\n",
            "PyWavelets versión: 1.8.0\n"
          ]
        }
      ],
      "source": [
        "def scales_to_frequencies(scales: np.ndarray, wavelet: str, sampling_rate: float) -> np.ndarray:\n",
        "    \"\"\"Convierte escalas wavelet a frecuencias.\"\"\"\n",
        "    if wavelet.startswith('cmor'):\n",
        "        # Para Complex Morlet: freq = (width * sampling_rate) / (2 * pi * scale)\n",
        "        # Extraer ancho de banda del nombre de la wavelet (formato: cmorB-C)\n",
        "        if '-' in wavelet:\n",
        "            bandwidth = float(wavelet.split('-')[0].replace('cmor', ''))\n",
        "        else:\n",
        "            bandwidth = CWT_WIDTH # Usar el ancho configurado globalmente si no está en el nombre\n",
        "        frequencies = (bandwidth * sampling_rate) / (2 * np.pi * scales)\n",
        "    else:\n",
        "        # Para otras wavelets, aproximación general\n",
        "        frequencies = sampling_rate / (2 * scales)\n",
        "    # Asegurarse de que las frecuencias estén en orden ascendente\n",
        "    sorted_indices = np.argsort(frequencies)\n",
        "    return frequencies[sorted_indices]\n",
        "\n",
        "\n",
        "def _compute_single_cwt_and_extract_features(args):\n",
        "    \"\"\"\n",
        "    Función auxiliar para calcular CWT de una sola señal y extraer características.\n",
        "    Usada para paralelización.\n",
        "    \"\"\"\n",
        "    signal, scales, wavelet, sampling_period, frequencies, freq_bands = args\n",
        "\n",
        "    # Calcular CWT\n",
        "    coefficients, _ = pywt.cwt(signal, scales, wavelet, sampling_period=sampling_period)\n",
        "    # coefficients tiene forma (n_scales, n_times)\n",
        "\n",
        "    # Extraer características\n",
        "    features = {}\n",
        "    power_spectrum = np.abs(coefficients)**2 # (n_scales, n_times)\n",
        "\n",
        "    # 1. Energía por banda de frecuencia\n",
        "    for band_name, (fmin, fmax) in freq_bands.items():\n",
        "        # Encontrar índices de escalas correspondientes a esta banda\n",
        "        # Las frecuencias corresponden a las escalas en orden original, no ordenado\n",
        "        original_frequencies = scales_to_frequencies(scales, wavelet, 1.0/sampling_period) # Re-calcular frecuencias para el orden original de escalas\n",
        "        band_mask = (original_frequencies >= fmin) & (original_frequencies <= fmax)\n",
        "        if not band_mask.any():\n",
        "             # Rellenar con NaNs o ceros si no hay escalas en esta banda para mantener dimensiones\n",
        "             # Usaremos NaN para distinguir de energía cero real\n",
        "             features[f'cwt_energy_{band_name}'] = np.nan\n",
        "             continue\n",
        "\n",
        "        # Energía promedio\n",
        "        band_coeffs = coefficients[band_mask, :]\n",
        "        energy = np.mean(np.abs(band_coeffs)**2)\n",
        "        features[f'cwt_energy_{band_name}'] = energy\n",
        "\n",
        "    # 2. Frecuencia dominante (promedio en el tiempo)\n",
        "    power_mean_time = np.mean(power_spectrum, axis=1) # (n_scales,)\n",
        "    if power_mean_time.sum() > 1e-10: # Evitar log(0) y división por cero\n",
        "        dominant_freq_idx = np.argmax(power_mean_time)\n",
        "        # Usar las frecuencias originales para el mapeo correcto\n",
        "        original_frequencies = scales_to_frequencies(scales, wavelet, 1.0/sampling_period)\n",
        "        dominant_frequency = original_frequencies[dominant_freq_idx]\n",
        "        features['cwt_dominant_freq'] = dominant_frequency\n",
        "    else:\n",
        "        features['cwt_dominant_freq'] = np.nan # O algún valor por defecto\n",
        "\n",
        "    # 3. Entropía espectral (promedio en el tiempo)\n",
        "    power_mean_time = np.mean(power_spectrum, axis=1) # (n_scales,)\n",
        "    power_norm = power_mean_time / (np.sum(power_mean_time) + 1e-10)\n",
        "    # Evitar log(0)\n",
        "    power_norm = power_norm[power_norm > 0]\n",
        "    spectral_entropy = -np.sum(power_norm * np.log(power_norm + 1e-10))\n",
        "    features['cwt_spectral_entropy'] = spectral_entropy\n",
        "\n",
        "    # Devolver un diccionario plano de características para esta señal\n",
        "    return features\n",
        "\n",
        "\n",
        "def compute_cwt_features_optimized(data: np.ndarray, sfreq: float, freq_bands: Dict[str, Tuple[float, float]],\n",
        "                                   n_jobs: int = -1, chunk_size: int = 100,\n",
        "                                   use_cache: bool = True, cache_dir: Optional[Path] = None) -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calcula coeficientes CWT y extrae características (OPTIMIZADO PARA MEMORIA).\n",
        "\n",
        "    Procesa los datos en chunks y extrae características directamente para evitar\n",
        "    almacenar la matriz completa de coeficientes CWT en memoria.\n",
        "\n",
        "    Args:\n",
        "        data: Array de forma (trials, channels, time)\n",
        "        sfreq: Frecuencia de muestreo\n",
        "        freq_bands: Diccionario con bandas de frecuencia {nombre: (fmin, fmax)}\n",
        "        n_jobs: Número de trabajos paralelos (-1 para todos los cores, None = N_JOBS)\n",
        "        chunk_size: Tamaño del chunk para procesamiento por lotes (número de señales/canales)\n",
        "        use_cache: Si True, intenta cargar/guardar cache\n",
        "        cache_dir: Directorio para cache (None = usar wavelet_reports/cwt_features_cache)\n",
        "\n",
        "    Returns:\n",
        "        Tuple de (DataFrame de características CWT, frecuencias CWT correspondientes a las escalas)\n",
        "    \"\"\"\n",
        "    n_trials, n_channels, n_times = data.shape\n",
        "\n",
        "    # Calcular frecuencias correspondientes a las escalas (para información, no para máscara)\n",
        "    frequencies = scales_to_frequencies(CWT_SCALES, CWT_WAVELET, sfreq)\n",
        "\n",
        "    # Configurar cache\n",
        "    if use_cache:\n",
        "        if cache_dir is None:\n",
        "            cache_dir = Path(\"wavelet_reports\") / \"cwt_features_cache\"\n",
        "            cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Crear nombre de cache basado en hash de parámetros y bandas\n",
        "        import hashlib\n",
        "        freq_bands_str = json.dumps(freq_bands, sort_keys=True)\n",
        "        params_hash = hashlib.md5(\n",
        "            f\"{data.shape}_{CWT_SCALES.tobytes()}_{CWT_WAVELET}_{sfreq}_{freq_bands_str}\".encode()\n",
        "        ).hexdigest()[:8]\n",
        "        cache_file = cache_dir / f\"cwt_features_{params_hash}.pkl\" # Usar pickle para DataFrame/dict\n",
        "\n",
        "        # Intentar cargar cache\n",
        "        if cache_file.exists():\n",
        "            print(f\"Cargando características CWT desde cache: {cache_file}\")\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                     cached_features = pickle.load(f)\n",
        "\n",
        "                # Verificar que el cache cargado tiene la estructura esperada\n",
        "                if isinstance(cached_features, pd.DataFrame) and cached_features.shape[0] == n_trials * n_channels:\n",
        "                     print(\"Cache de características CWT cargado exitosamente!\")\n",
        "                     # Las características están a nivel de trial-canal, necesitamos reestructurarlas\n",
        "                     # Mapear de nuevo a (trials, channels, features) si es necesario, o mantener plano\n",
        "                     # Para BoF, el formato plano trial*channel, feature es útil.\n",
        "                     # Asegurarnos de que las columnas corresponden a las características esperadas\n",
        "                     expected_feature_names = [\n",
        "                         f'cwt_energy_{band_name}' for band_name in freq_bands.keys()\n",
        "                     ] + [\n",
        "                         'cwt_dominant_freq', 'cwt_spectral_entropy'\n",
        "                     ]\n",
        "                     if all(name in cached_features.columns for name in expected_feature_names):\n",
        "                          print(\"Estructura de cache de características verificada.\")\n",
        "                          # Aquí cached_features ya es un DataFrame plano (trial*channel, feature_name)\n",
        "                          # Podemos devolverlo directamente o reestructurarlo si el paso siguiente lo necesita\n",
        "                          # Para el paso de combinación de características, el formato plano es conveniente\n",
        "                          # Renombrar columnas para incluir canal si no lo están\n",
        "                          if not any(str(c).endswith('_ch') for c in cached_features.columns):\n",
        "                              # Asumimos que el cache plano tiene el orden trial1_ch1, trial1_ch2, ..., trialN_chM\n",
        "                              # Necesitamos reconstruir los nombres completos si el cache es plano\n",
        "                              # Re-estructurar para mantener la lógica de extract_cwt_features si es necesario\n",
        "                              # Sin embargo, el nuevo enfoque es extraer directo para cada señal.\n",
        "                              # Vamos a devolver el DataFrame plano temporalmente y ajustar el paso siguiente.\n",
        "                              # O mejor, reestructurar a la forma (trials, channels, features) aquí\n",
        "                              print(\"Reestructurando cache cargado a formato (trials, channels, features)...\")\n",
        "                              n_extracted_features = len(expected_feature_names)\n",
        "                              features_array_reshaped = cached_features.values.reshape(n_trials, n_channels, n_extracted_features)\n",
        "\n",
        "                              # Crear un diccionario similar al output de extract_cwt_features\n",
        "                              features_dict_reshaped = {}\n",
        "                              for i, feature_name in enumerate(expected_feature_names):\n",
        "                                  features_dict_reshaped[feature_name] = features_array_reshaped[:, :, i]\n",
        "\n",
        "                              # Devolver el diccionario reestructurado\n",
        "                              return features_dict_reshaped, frequencies\n",
        "\n",
        "                          # Si las columnas ya tienen nombres de canal, el cache es plano pero ya con nombres correctos\n",
        "                          # En este caso, el formato plano trial, feature_name_channel ya está listo\n",
        "                          # Para el paso de combinación, necesitamos el diccionario original (feature_name, array(trials, channels))\n",
        "                          # Necesitamos reestructurar el DataFrame plano a un diccionario de arrays 2D\n",
        "                          print(\"Reestructurando cache cargado a diccionario de arrays (trials, channels)...\")\n",
        "                          features_dict_reshaped = {}\n",
        "                          for col in cached_features.columns:\n",
        "                              # Extraer nombre de característica original y canal\n",
        "                              parts = col.rsplit('_', 1)\n",
        "                              if len(parts) == 2 and parts[-1].startswith('ch'): # Asumimos formato feature_name_chX\n",
        "                                  feature_name = parts[0]\n",
        "                                  # channel_name = parts[1] # No necesitamos el nombre del canal aquí\n",
        "                                  # Reestructurar la columna a (trials, channels)\n",
        "                                  # Esto requiere asumir un orden específico en el DataFrame plano (trial1_ch1, trial1_ch2, ..., trialN_chM)\n",
        "                                  # O que las columnas estén en el orden correcto (feature1_ch1, feature1_ch2, ..., featureN_chM)\n",
        "                                  # Asumimos el segundo caso: columnas agrupadas por feature, luego por canal\n",
        "                                  # En este caso, necesitamos agrupar columnas por feature_name\n",
        "                                  if feature_name not in features_dict_reshaped:\n",
        "                                      features_dict_reshaped[feature_name] = np.zeros((n_trials, n_channels))\n",
        "                                  # Encontrar el índice del canal a partir del nombre de la columna\n",
        "                                  # Esto es complicado si no tenemos el mapeo exacto de columnas a (trial, channel)\n",
        "                                  # El enfoque anterior de reconstruir el array 3D y luego el dict es más seguro.\n",
        "                                  pass # Revertir al enfoque anterior de array 3D si es posible\n",
        "\n",
        "                              # Si las columnas son solo feature_name y el cache es plano (trial*channel, feature)\n",
        "                              # Necesitamos reconstruir el array 3D (trials, channels, features) y luego el dict\n",
        "                              # Este caso ya fue manejado arriba.\n",
        "\n",
        "                          # Si el cache cargado ya es un diccionario con la forma esperada (feature_name, array(trials, channels))\n",
        "                          # O (feature_name, array(trials, channels, scales))\n",
        "                          if isinstance(cached_features, dict):\n",
        "                               print(\"Cache cargado es un diccionario, verificando estructura...\")\n",
        "                               is_valid_cache = True\n",
        "                               for feature_name, arr in cached_features.items():\n",
        "                                   if not isinstance(arr, np.ndarray) or arr.shape[0] != n_trials or arr.shape[1] != n_channels:\n",
        "                                       is_valid_cache = False\n",
        "                                       break\n",
        "                               if is_valid_cache:\n",
        "                                   print(\"Estructura del diccionario de cache verificada.\")\n",
        "                                   # Asegurarse de que solo se devuelven las características esperadas\n",
        "                                   filtered_features = {\n",
        "                                       name: arr for name, arr in cached_features.items()\n",
        "                                       if name in [f'cwt_energy_{band_name}' for band_name in freq_bands.keys()] + ['cwt_dominant_freq', 'cwt_spectral_entropy']\n",
        "                                   }\n",
        "                                   if len(filtered_features) == len([f'cwt_energy_{band_name}' for band_name in freq_bands.keys()]) + 2: # Verificar que todas las esperadas están presentes\n",
        "                                        print(\"Todas las características esperadas encontradas en el diccionario de cache.\")\n",
        "                                        return filtered_features, frequencies\n",
        "                                   else:\n",
        "                                        print(\"El diccionario de cache no contiene todas las características esperadas, recalculando...\")\n",
        "                               else:\n",
        "                                   print(\"El diccionario de cache tiene estructuras incorrectas, recalculando...\")\n",
        "\n",
        "                          # Si llegamos aquí, el cache no tiene el formato esperado, recalcular.\n",
        "                          print(\"El formato del cache cargado no coincide con lo esperado, recalculando...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error al cargar cache: {e}, recalculando...\")\n",
        "    else:\n",
        "        cache_file = None\n",
        "\n",
        "    # Usar n_jobs de configuración global si es -1\n",
        "    if n_jobs == -1:\n",
        "        try:\n",
        "            n_jobs = N_JOBS\n",
        "        except NameError:\n",
        "            n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
        "\n",
        "    print(\"Calculando coeficientes CWT y extrayendo características (versión optimizada)...\")\n",
        "    print(f\"  - Total de señales a procesar: {n_trials * n_channels}\")\n",
        "    print(f\"  - Procesadores a utilizar: {n_jobs if n_jobs > 0 else 'todos'}\")\n",
        "    print(f\"  - Chunk size: {chunk_size}\")\n",
        "\n",
        "    # Preparar argumentos para paralelización\n",
        "    args_list = []\n",
        "    indices = []  # Para mapear resultados (trial_idx, ch_idx)\n",
        "\n",
        "    # Aplanar los datos para procesar cada señal (trial, channel) de forma independiente\n",
        "    data_flat = data.reshape(n_trials * n_channels, n_times) # (total_signals, time)\n",
        "\n",
        "    for i in range(n_trials * n_channels):\n",
        "        trial_idx = i // n_channels\n",
        "        ch_idx = i % n_channels\n",
        "        signal = data_flat[i, :]\n",
        "        args_list.append((signal, CWT_SCALES, CWT_WAVELET, 1.0/sfreq, frequencies, freq_bands))\n",
        "        indices.append((trial_idx, ch_idx))\n",
        "\n",
        "    # Procesar en paralelo con barra de progreso y backend threading (más eficiente para I/O como CWT)\n",
        "    # Usar 'loky' o 'multiprocessing' si threading da problemas\n",
        "    results = Parallel(n_jobs=n_jobs, backend='threading', batch_size=chunk_size)(\n",
        "        delayed(_compute_single_cwt_and_extract_features)(args) for args in tqdm(args_list, desc=\"Procesando CWT y extrayendo características\")\n",
        "    )\n",
        "\n",
        "    # Reestructurar resultados en un DataFrame plano (trial*channel, feature_name)\n",
        "    # El orden de los resultados corresponde al orden de args_list\n",
        "    features_list = []\n",
        "    for feature_dict in results:\n",
        "        features_list.append(feature_dict)\n",
        "\n",
        "    # Crear DataFrame a partir de la lista de diccionarios\n",
        "    features_df_flat = pd.DataFrame(features_list)\n",
        "\n",
        "    # Reestructurar el DataFrame plano a un diccionario de arrays 2D (feature_name, array(trials, channels))\n",
        "    # Esto asume que las columnas en features_df_flat corresponden a las características esperadas\n",
        "    extracted_features_dict = {}\n",
        "    expected_feature_names = list(features_list[0].keys()) # Obtener nombres de características del primer resultado\n",
        "\n",
        "    for feature_name in expected_feature_names:\n",
        "         # Extraer columna del DataFrame plano y reestructurar a (trials, channels)\n",
        "         feature_data_flat = features_df_flat[feature_name].values # (n_trials * n_channels,)\n",
        "         feature_data_reshaped = feature_data_flat.reshape(n_trials, n_channels) # (trials, channels)\n",
        "         extracted_features_dict[feature_name] = feature_data_reshaped\n",
        "\n",
        "\n",
        "    # Guardar en cache si está habilitado\n",
        "    if use_cache and cache_file is not None:\n",
        "        try:\n",
        "            # Guardar el diccionario reestructurado\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(extracted_features_dict, f)\n",
        "            print(f\"Características CWT guardadas en cache: {cache_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al guardar cache: {e}\")\n",
        "\n",
        "    return extracted_features_dict, frequencies\n",
        "\n",
        "\n",
        "print(\"Funciones auxiliares para wavelets definidas (versión optimizada)\")\n",
        "print(f\"PyWavelets versión: {pywt.__version__}\") # Asegurarse de que la versión se imprime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euliYbEA8veD"
      },
      "source": [
        "## Análisis CWT (Transformada Wavelet Continua)\n",
        "\n",
        "Calculamos los coeficientes CWT para análisis tiempo-frecuencia:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH3KDDtR8veE",
        "outputId": "6e3833a5-0779-406f-948d-ddcf2d5874cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando análisis CWT y extracción de características optimizada...\n",
            "Datos de entrada: (880, 64, 1152) (trials, channels, time)\n",
            "Frecuencia de muestreo: 128.0 Hz\n",
            "Bandas de frecuencia para extracción: ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
            "Calculando coeficientes CWT y extrayendo características (versión optimizada)...\n",
            "  - Total de señales a procesar: 56320\n",
            "  - Procesadores a utilizar: 1\n",
            "  - Chunk size: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando CWT y extrayendo características: 100%|██████████| 56320/56320 [34:57<00:00, 26.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error al guardar cache: name 'pickle' is not defined\n",
            "\n",
            "Análisis CWT y extracción de características completados:\n",
            "Características CWT extraídas:\n",
            "  - cwt_energy_delta: (880, 64)\n",
            "  - cwt_energy_theta: (880, 64)\n",
            "  - cwt_energy_alpha: (880, 64)\n",
            "  - cwt_energy_beta: (880, 64)\n",
            "  - cwt_energy_gamma: (880, 64)\n",
            "  - cwt_dominant_freq: (880, 64)\n",
            "  - cwt_spectral_entropy: (880, 64)\n",
            "Frecuencias CWT: 50 puntos\n",
            "Rango de frecuencias: 0.3 - 32.2 Hz\n",
            "\n",
            "Bandas de frecuencia CWT (basado en escalas y sfreq):\n",
            "  - delta (1-4 Hz): 14 escalas\n",
            "  - theta (4-8 Hz): 8 escalas\n",
            "  - alpha (8-13 Hz): 5 escalas\n",
            "  - beta (13-30 Hz): 9 escalas\n",
            "  - gamma (30-100 Hz): 1 escalas\n"
          ]
        }
      ],
      "source": [
        "# Calcular coeficientes CWT y extraer características usando la función optimizada\n",
        "print(\"Iniciando análisis CWT y extracción de características optimizada...\")\n",
        "print(f\"Datos de entrada: {X.shape} (trials, channels, time)\")\n",
        "print(f\"Frecuencia de muestreo: {sfreq} Hz\")\n",
        "print(f\"Bandas de frecuencia para extracción: {list(FREQ_BANDS.keys())}\")\n",
        "\n",
        "\n",
        "# Las características CWT más importantes para BoF se extraen directamente aquí\n",
        "# Definimos las características clave que queremos obtener de CWT para el paso de combinación\n",
        "cwt_key_features_to_extract = {\n",
        "    'cwt_energy_delta': FREQ_BANDS['delta'],\n",
        "    'cwt_energy_theta': FREQ_BANDS['theta'],\n",
        "    'cwt_energy_alpha': FREQ_BANDS['alpha'],\n",
        "    'cwt_energy_beta': FREQ_BANDS['beta'],\n",
        "    'cwt_energy_gamma': FREQ_BANDS['gamma'],\n",
        "    # Frecuencia dominante y entropía espectral se extraen si es posible\n",
        "    # No necesitan una banda de frecuencia específica, se calculan sobre todas las escalas\n",
        "}\n",
        "\n",
        "\n",
        "cwt_features, cwt_frequencies = compute_cwt_features_optimized(\n",
        "    X, sfreq,\n",
        "    freq_bands=FREQ_BANDS, # Pasar las bandas de frecuencia para extracción\n",
        "    n_jobs=-1,           # Usar procesamiento paralelo (-1 = N_JOBS global)\n",
        "    chunk_size=50,       # Procesar 50 señales por lote\n",
        "    use_cache=True       # Habilitar cache\n",
        ")\n",
        "\n",
        "print(f\"\\nAnálisis CWT y extracción de características completados:\")\n",
        "print(f\"Características CWT extraídas:\")\n",
        "for feature_name, feature_array in cwt_features.items():\n",
        "    print(f\"  - {feature_name}: {feature_array.shape}\")\n",
        "\n",
        "print(f\"Frecuencias CWT: {len(cwt_frequencies)} puntos\")\n",
        "print(f\"Rango de frecuencias: {cwt_frequencies[0]:.1f} - {cwt_frequencies[-1]:.1f} Hz\")\n",
        "\n",
        "\n",
        "# Mostrar información sobre las bandas de frecuencia (basado en las frecuencias calculadas)\n",
        "print(f\"\\nBandas de frecuencia CWT (basado en escalas y sfreq):\")\n",
        "for band_name, (fmin, fmax) in FREQ_BANDS.items():\n",
        "    # Usar cwt_frequencies que ya está ordenado\n",
        "    mask = (cwt_frequencies >= fmin) & (cwt_frequencies <= fmax)\n",
        "    n_scales = mask.sum()\n",
        "    if n_scales > 0:\n",
        "        print(f\"  - {band_name} ({fmin}-{fmax} Hz): {n_scales} escalas\")\n",
        "    else:\n",
        "        print(f\"  - {band_name} ({fmin}-{fmax} Hz): fuera del rango\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEf12v3Z8veE"
      },
      "source": [
        "### Extracción de Características CWT\n",
        "\n",
        "Extraemos características estadísticas de los coeficientes CWT para BoF:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUcQnQcF8veE",
        "outputId": "6891ae53-b12b-4d18-c1fd-32f82ab794d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrayendo características CWT...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Esta celda ya no es necesaria ya que la extracción de características CWT se realiza\n",
        "# directamente dentro de compute_cwt_features_optimized para ahorrar memoria.\n",
        "# El resultado de compute_cwt_features_optimized ya es el diccionario cwt_features.\n",
        "\n",
        "# print(\"Extrayendo características CWT...\")\n",
        "# cwt_features = extract_cwt_features(cwt_coeffs, cwt_frequencies)\n",
        "\n",
        "# print(f\"\\nCaracterísticas CWT extraídas:\")\n",
        "# for feature_name, feature_array in cwt_features.items():\n",
        "#     print(f\"  - {feature_name}: {feature_array.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfL0J-178veF"
      },
      "source": [
        "## Análisis DWT (Transformada Wavelet Discreta)\n",
        "\n",
        "Implementamos análisis DWT para descomposición multiresolución:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-UtpIji8veF",
        "outputId": "463daeb7-fe3f-4394-8833-97314f3c724b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando análisis DWT...\n",
            "Datos de entrada: (880, 64, 1152) (trials, channels, time)\n",
            "Wavelet: db4, Niveles: 6\n",
            "Calculando coeficientes DWT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trials: 100%|██████████| 880/880 [00:08<00:00, 105.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DWT completado:\n",
            "Coeficientes DWT por nivel:\n",
            "  - level_0: (880, 64, 24)\n",
            "  - level_1: (880, 64, 24)\n",
            "  - level_2: (880, 64, 42)\n",
            "  - level_3: (880, 64, 78)\n",
            "  - level_4: (880, 64, 150)\n",
            "  - level_5: (880, 64, 293)\n",
            "  - level_6: (880, 64, 579)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def compute_dwt_coefficients(data: np.ndarray, wavelet: str, levels: int) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calcula coeficientes DWT para todos los canales y trials.\n",
        "\n",
        "    Args:\n",
        "        data: Array de forma (trials, channels, time)\n",
        "        wavelet: Tipo de wavelet (ej. 'db4')\n",
        "        levels: Número de niveles de descomposición\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con coeficientes DWT por nivel\n",
        "    \"\"\"\n",
        "    n_trials, n_channels, n_times = data.shape\n",
        "\n",
        "    # Inicializar diccionario para coeficientes\n",
        "    dwt_coeffs = {}\n",
        "\n",
        "    print(\"Calculando coeficientes DWT...\")\n",
        "    for trial_idx in tqdm(range(n_trials), desc=\"Trials\"):\n",
        "        for ch_idx in range(n_channels):\n",
        "            signal = data[trial_idx, ch_idx, :]\n",
        "\n",
        "            # Calcular DWT\n",
        "            coeffs = pywt.wavedec(signal, wavelet, level=levels)\n",
        "\n",
        "            # Guardar coeficientes por nivel\n",
        "            for level, coeff in enumerate(coeffs):\n",
        "                key = f'level_{level}'\n",
        "                if key not in dwt_coeffs:\n",
        "                    dwt_coeffs[key] = np.zeros((n_trials, n_channels, len(coeff)))\n",
        "                dwt_coeffs[key][trial_idx, ch_idx, :] = coeff\n",
        "\n",
        "    return dwt_coeffs\n",
        "\n",
        "# Calcular coeficientes DWT\n",
        "print(\"Iniciando análisis DWT...\")\n",
        "print(f\"Datos de entrada: {X.shape} (trials, channels, time)\")\n",
        "print(f\"Wavelet: {DWT_WAVELET}, Niveles: {DWT_LEVELS}\")\n",
        "\n",
        "dwt_coeffs = compute_dwt_coefficients(X, DWT_WAVELET, DWT_LEVELS)\n",
        "\n",
        "print(f\"\\nDWT completado:\")\n",
        "print(f\"Coeficientes DWT por nivel:\")\n",
        "for level_key, coeff_array in dwt_coeffs.items():\n",
        "    print(f\"  - {level_key}: {coeff_array.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_iaRC2l8veF"
      },
      "source": [
        "### Extracción de Características DWT\n",
        "\n",
        "Extraemos características estadísticas de los coeficientes DWT:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJeymvH18veF",
        "outputId": "2b3cda91-1731-43ec-d08d-9c52f259c099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extrayendo características DWT...\n",
            "  - Energía level_0: (880, 64)\n",
            "  - Energía level_1: (880, 64)\n",
            "  - Energía level_2: (880, 64)\n",
            "  - Energía level_3: (880, 64)\n",
            "  - Energía level_4: (880, 64)\n",
            "  - Energía level_5: (880, 64)\n",
            "  - Energía level_6: (880, 64)\n",
            "  - Estadísticas level_0: media, std, max\n",
            "  - Estadísticas level_1: media, std, max\n",
            "  - Estadísticas level_2: media, std, max\n",
            "  - Estadísticas level_3: media, std, max\n",
            "  - Estadísticas level_4: media, std, max\n",
            "  - Estadísticas level_5: media, std, max\n",
            "  - Estadísticas level_6: media, std, max\n",
            "  - Relación de energía level0/level1: (880, 64)\n",
            "\n",
            "Características DWT extraídas:\n",
            "  - dwt_energy_level_0: (880, 64)\n",
            "  - dwt_energy_level_1: (880, 64)\n",
            "  - dwt_energy_level_2: (880, 64)\n",
            "  - dwt_energy_level_3: (880, 64)\n",
            "  - dwt_energy_level_4: (880, 64)\n",
            "  - dwt_energy_level_5: (880, 64)\n",
            "  - dwt_energy_level_6: (880, 64)\n",
            "  - dwt_mean_level_0: (880, 64)\n",
            "  - dwt_std_level_0: (880, 64)\n",
            "  - dwt_max_level_0: (880, 64)\n",
            "  - dwt_mean_level_1: (880, 64)\n",
            "  - dwt_std_level_1: (880, 64)\n",
            "  - dwt_max_level_1: (880, 64)\n",
            "  - dwt_mean_level_2: (880, 64)\n",
            "  - dwt_std_level_2: (880, 64)\n",
            "  - dwt_max_level_2: (880, 64)\n",
            "  - dwt_mean_level_3: (880, 64)\n",
            "  - dwt_std_level_3: (880, 64)\n",
            "  - dwt_max_level_3: (880, 64)\n",
            "  - dwt_mean_level_4: (880, 64)\n",
            "  - dwt_std_level_4: (880, 64)\n",
            "  - dwt_max_level_4: (880, 64)\n",
            "  - dwt_mean_level_5: (880, 64)\n",
            "  - dwt_std_level_5: (880, 64)\n",
            "  - dwt_max_level_5: (880, 64)\n",
            "  - dwt_mean_level_6: (880, 64)\n",
            "  - dwt_std_level_6: (880, 64)\n",
            "  - dwt_max_level_6: (880, 64)\n",
            "  - dwt_energy_ratio_level0_level1: (880, 64)\n"
          ]
        }
      ],
      "source": [
        "def extract_dwt_features(dwt_coeffs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extrae características de los coeficientes DWT.\n",
        "\n",
        "    Args:\n",
        "        dwt_coeffs: Diccionario con coeficientes DWT por nivel\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con características extraídas\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    print(\"Extrayendo características DWT...\")\n",
        "\n",
        "    # 1. Energía por nivel de descomposición\n",
        "    for level_key, coeff_array in dwt_coeffs.items():\n",
        "        # Energía promedio por trial y canal\n",
        "        energy = np.mean(coeff_array**2, axis=2)  # (trials, channels)\n",
        "        features[f'dwt_energy_{level_key}'] = energy\n",
        "        print(f\"  - Energía {level_key}: {energy.shape}\")\n",
        "\n",
        "    # 2. Estadísticas por nivel\n",
        "    for level_key, coeff_array in dwt_coeffs.items():\n",
        "        # Media por trial y canal\n",
        "        mean_coeffs = np.mean(coeff_array, axis=2)  # (trials, channels)\n",
        "        features[f'dwt_mean_{level_key}'] = mean_coeffs\n",
        "\n",
        "        # Desviación estándar por trial y canal\n",
        "        std_coeffs = np.std(coeff_array, axis=2)  # (trials, channels)\n",
        "        features[f'dwt_std_{level_key}'] = std_coeffs\n",
        "\n",
        "        # Máximo absoluto por trial y canal\n",
        "        max_coeffs = np.max(np.abs(coeff_array), axis=2)  # (trials, channels)\n",
        "        features[f'dwt_max_{level_key}'] = max_coeffs\n",
        "\n",
        "        print(f\"  - Estadísticas {level_key}: media, std, max\")\n",
        "\n",
        "    # 3. Relación de energía entre niveles (aproximación de bandas de frecuencia)\n",
        "    if 'level_0' in dwt_coeffs and 'level_1' in dwt_coeffs:\n",
        "        # Relación entre aproximación y detalle\n",
        "        energy_level_0 = np.mean(dwt_coeffs['level_0']**2, axis=2)\n",
        "        energy_level_1 = np.mean(dwt_coeffs['level_1']**2, axis=2)\n",
        "        energy_ratio = energy_level_0 / (energy_level_1 + 1e-10)\n",
        "        features['dwt_energy_ratio_level0_level1'] = energy_ratio\n",
        "        print(f\"  - Relación de energía level0/level1: {energy_ratio.shape}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extraer características DWT\n",
        "dwt_features = extract_dwt_features(dwt_coeffs)\n",
        "\n",
        "print(f\"\\nCaracterísticas DWT extraídas:\")\n",
        "for feature_name, feature_array in dwt_features.items():\n",
        "    print(f\"  - {feature_name}: {feature_array.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpkL1jtQ8veF"
      },
      "source": [
        "## Preparación de Características para BoF\n",
        "\n",
        "Combinamos todas las características extraídas y las preparamos para Bag of Features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meeFFQFm8veF",
        "outputId": "a094ce16-39fc-48aa-ca72-cfa815a937c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combinando características para BoF...\n",
            "Total de características extraídas: 36\n",
            "Matriz de características final:\n",
            "  - Trials: 880\n",
            "  - Características totales: 2304\n",
            "  - Primeras 10 características: ['cwt_energy_delta_FC5', 'cwt_energy_delta_FC3', 'cwt_energy_delta_FC1', 'cwt_energy_delta_FCZ', 'cwt_energy_delta_FC2', 'cwt_energy_delta_FC4', 'cwt_energy_delta_FC6', 'cwt_energy_delta_C5', 'cwt_energy_delta_C3', 'cwt_energy_delta_C1']\n",
            "Características normalizadas: (880, 2304)\n",
            "Media después de normalización: nan\n",
            "Desviación estándar después de normalización: nan\n"
          ]
        }
      ],
      "source": [
        "# Combinar todas las características\n",
        "print(\"Combinando características para BoF...\")\n",
        "\n",
        "# Combinar características CWT y DWT\n",
        "all_features = {}\n",
        "all_features.update(cwt_features)\n",
        "all_features.update(dwt_features)\n",
        "\n",
        "print(f\"Total de características extraídas: {len(all_features)}\")\n",
        "\n",
        "# Crear matriz de características para BoF\n",
        "feature_names = []\n",
        "feature_matrices = []\n",
        "\n",
        "for feature_name, feature_array in all_features.items():\n",
        "    # Reshape para tener una matriz 2D (samples, features)\n",
        "    if len(feature_array.shape) == 2:  # (trials, channels)\n",
        "        # Cada canal es una característica\n",
        "        for ch_idx, ch_name in enumerate(ch_names):\n",
        "            feature_names.append(f\"{feature_name}_{ch_name}\")\n",
        "            feature_matrices.append(feature_array[:, ch_idx])\n",
        "    elif len(feature_array.shape) == 3:  # (trials, channels, scales/levels)\n",
        "        # Cada combinación canal-escala es una característica\n",
        "        for ch_idx, ch_name in enumerate(ch_names):\n",
        "            for scale_idx in range(feature_array.shape[2]):\n",
        "                feature_names.append(f\"{feature_name}_{ch_name}_scale{scale_idx}\")\n",
        "                feature_matrices.append(feature_array[:, ch_idx, scale_idx])\n",
        "\n",
        "# Crear matriz final de características\n",
        "X_features = np.column_stack(feature_matrices)  # (trials, total_features)\n",
        "\n",
        "print(f\"Matriz de características final:\")\n",
        "print(f\"  - Trials: {X_features.shape[0]}\")\n",
        "print(f\"  - Características totales: {X_features.shape[1]}\")\n",
        "print(f\"  - Primeras 10 características: {feature_names[:10]}\")\n",
        "\n",
        "# Normalizar características\n",
        "scaler = StandardScaler()\n",
        "X_features_normalized = scaler.fit_transform(X_features)\n",
        "\n",
        "print(f\"Características normalizadas: {X_features_normalized.shape}\")\n",
        "print(f\"Media después de normalización: {X_features_normalized.mean():.6f}\")\n",
        "print(f\"Desviación estándar después de normalización: {X_features_normalized.std():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMVNkMtW8veG"
      },
      "source": [
        "## Guardado de Archivos para BoF\n",
        "\n",
        "Guardamos las características y metadatos necesarios para implementar Bag of Features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7BnrDXf8veG",
        "outputId": "59db7c00-dc90-4f57-d6e4-d3c1f14ede55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Características guardadas: /content/wavelet_reports/wavelet_features.npy\n",
            "Nombres de características guardados: /content/wavelet_reports/feature_names.txt\n",
            "Información de canales guardada: /content/wavelet_reports/channel_info.csv\n",
            "Información de sujetos guardada: /content/wavelet_reports/subjects_info.csv\n",
            "Configuración guardada: /content/wavelet_reports/wavelet_config.json\n",
            "\n",
            "Resumen de archivos generados:\n",
            "  - wavelet_features.npy: Características normalizadas ((880, 2304))\n",
            "  - feature_names.txt: Nombres de características (2304 características)\n",
            "  - channel_info.csv: Información de canales (64 canales)\n",
            "  - subjects_info.csv: Información de sujetos (40 archivos)\n",
            "  - wavelet_config.json: Parámetros de configuración\n"
          ]
        }
      ],
      "source": [
        "# Guardar características normalizadas para BoF\n",
        "features_file = wavelet_output_dir / \"wavelet_features.npy\"\n",
        "np.save(features_file, X_features_normalized)\n",
        "print(f\"Características guardadas: {features_file.resolve()}\")\n",
        "\n",
        "# Guardar nombres de características\n",
        "feature_names_file = wavelet_output_dir / \"feature_names.txt\"\n",
        "with open(feature_names_file, 'w') as f:\n",
        "    for name in feature_names:\n",
        "        f.write(f\"{name}\\n\")\n",
        "print(f\"Nombres de características guardados: {feature_names_file.resolve()}\")\n",
        "\n",
        "# Guardar información de canales\n",
        "channel_info = pd.DataFrame({\n",
        "    'channel_index': range(len(ch_names)),\n",
        "    'channel_name': ch_names,\n",
        "    'region': ['unknown'] * len(ch_names)  # Se puede mejorar con mapeo de regiones\n",
        "})\n",
        "\n",
        "# Mapear regiones basado en prefijos\n",
        "for idx, ch_name in enumerate(ch_names):\n",
        "    ch_upper = ch_name.upper()\n",
        "    for region, prefixes in REGION_PREFIXES.items():\n",
        "        if any(ch_upper.startswith(p) for p in prefixes):\n",
        "            channel_info.loc[idx, 'region'] = region\n",
        "            break\n",
        "\n",
        "channel_info_file = wavelet_output_dir / \"channel_info.csv\"\n",
        "channel_info.to_csv(channel_info_file, index=False)\n",
        "print(f\"Información de canales guardada: {channel_info_file.resolve()}\")\n",
        "\n",
        "# Guardar información de sujetos y tareas\n",
        "subjects_df = pd.DataFrame(subjects_info)\n",
        "subjects_file = wavelet_output_dir / \"subjects_info.csv\"\n",
        "subjects_df.to_csv(subjects_file, index=False)\n",
        "print(f\"Información de sujetos guardada: {subjects_file.resolve()}\")\n",
        "\n",
        "# Guardar parámetros de configuración\n",
        "config_info = {\n",
        "    'cwt_scales': CWT_SCALES.tolist(),\n",
        "    'cwt_wavelet': CWT_WAVELET,\n",
        "    'cwt_width': CWT_WIDTH,\n",
        "    'dwt_wavelet': DWT_WAVELET,\n",
        "    'dwt_levels': DWT_LEVELS,\n",
        "    'freq_bands': FREQ_BANDS,\n",
        "    'sampling_rate': sfreq,\n",
        "    'n_trials': N,\n",
        "    'n_channels': C,\n",
        "    'n_timepoints': T,\n",
        "    'feature_dimensions': X_features_normalized.shape[1]\n",
        "}\n",
        "\n",
        "config_file = wavelet_output_dir / \"wavelet_config.json\"\n",
        "import json\n",
        "with open(config_file, 'w') as f:\n",
        "    json.dump(config_info, f, indent=2)\n",
        "print(f\"Configuración guardada: {config_file.resolve()}\")\n",
        "\n",
        "print(f\"\\nResumen de archivos generados:\")\n",
        "print(f\"  - {features_file.name}: Características normalizadas ({X_features_normalized.shape})\")\n",
        "print(f\"  - {feature_names_file.name}: Nombres de características ({len(feature_names)} características)\")\n",
        "print(f\"  - {channel_info_file.name}: Información de canales ({len(ch_names)} canales)\")\n",
        "print(f\"  - {subjects_file.name}: Información de sujetos ({len(subjects_info)} archivos)\")\n",
        "print(f\"  - {config_file.name}: Parámetros de configuración\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi_3rXPz8veG"
      },
      "source": [
        "## Preparación de Datos para Bag of Features (BoF)\n",
        "\n",
        "Esta sección prepara específicamente los datos que necesita el algoritmo Bag of Features para clasificación:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rRpDdf58veG",
        "outputId": "744bedc1-3fbd-4ffc-b353-3a90ef47bbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparando datos específicos para Bag of Features...\n",
            "Directorio BoF: /content/bof_data\n",
            "\n",
            "1. Creando etiquetas de clase...\n",
            "  - Etiquetas creadas: (880,)\n",
            "  - Clase 0 (left): 0 trials\n",
            "  - Clase 1 (right): 880 trials\n",
            "  - Etiquetas guardadas en: bof_data/y_labels.npy\n"
          ]
        }
      ],
      "source": [
        "# Crear directorio específico para datos de BoF\n",
        "bof_data_dir = Path(\"bof_data\")\n",
        "bof_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Preparando datos específicos para Bag of Features...\")\n",
        "print(f\"Directorio BoF: {bof_data_dir.resolve()}\")\n",
        "\n",
        "# 1. Crear etiquetas de clase basadas en la tarea (left/right)\n",
        "print(\"\\n1. Creando etiquetas de clase...\")\n",
        "\n",
        "# Crear array de etiquetas basado en subjects_info\n",
        "y_labels = []\n",
        "trial_to_subject = []  # Mapeo de trial a sujeto\n",
        "trial_to_task = []     # Mapeo de trial a tarea\n",
        "\n",
        "trial_idx = 0\n",
        "for subject_info in subjects_info:\n",
        "    n_trials = subject_info['n_trials']\n",
        "    task = subject_info['task']\n",
        "    subject = subject_info['subject']\n",
        "\n",
        "    # Etiquetas: 0 = left, 1 = right\n",
        "    label = 0 if task == 'left' else 1\n",
        "\n",
        "    for _ in range(n_trials):\n",
        "        y_labels.append(label)\n",
        "        trial_to_subject.append(subject)\n",
        "        trial_to_task.append(task)\n",
        "        trial_idx += 1\n",
        "\n",
        "y_labels = np.array(y_labels)\n",
        "print(f\"  - Etiquetas creadas: {y_labels.shape}\")\n",
        "print(f\"  - Clase 0 (left): {np.sum(y_labels == 0)} trials\")\n",
        "print(f\"  - Clase 1 (right): {np.sum(y_labels == 1)} trials\")\n",
        "\n",
        "# Guardar etiquetas\n",
        "np.save(bof_data_dir / \"y_labels.npy\", y_labels)\n",
        "np.save(bof_data_dir / \"trial_to_subject.npy\", np.array(trial_to_subject))\n",
        "np.save(bof_data_dir / \"trial_to_task.npy\", np.array(trial_to_task))\n",
        "\n",
        "print(f\"  - Etiquetas guardadas en: {bof_data_dir / 'y_labels.npy'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7d6v-0P8veG",
        "outputId": "bed077b3-21f1-4c8c-b621-554d9b048c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Preparando características para BoF...\n",
            "  - cwt_energy_alpha: (880, 64)\n",
            "  - cwt_energy_beta: (880, 64)\n",
            "  - cwt_dominant_freq: (880, 64)\n",
            "  - cwt_spectral_entropy: (880, 64)\n",
            "  - dwt_energy_level_0: (880, 64)\n",
            "  - dwt_energy_level_1: (880, 64)\n",
            "  - dwt_energy_level_2: (880, 64)\n",
            "  - dwt_mean_level_0: (880, 64)\n",
            "  - dwt_std_level_0: (880, 64)\n",
            "\n",
            "Total de características seleccionadas: 9\n",
            "Matriz de características BoF:\n",
            "  - Trials: 880\n",
            "  - Características: 576\n",
            "  - Primeras 10 características: ['cwt_energy_alpha_FC5', 'cwt_energy_alpha_FC3', 'cwt_energy_alpha_FC1', 'cwt_energy_alpha_FCZ', 'cwt_energy_alpha_FC2', 'cwt_energy_alpha_FC4', 'cwt_energy_alpha_FC6', 'cwt_energy_alpha_C5', 'cwt_energy_alpha_C3', 'cwt_energy_alpha_C1']\n",
            "Características BoF normalizadas: (880, 576)\n",
            "Media después de normalización: nan\n",
            "Desviación estándar: nan\n"
          ]
        }
      ],
      "source": [
        "# 2. Preparar características específicas para BoF\n",
        "print(\"\\n2. Preparando características para BoF...\")\n",
        "\n",
        "# Seleccionar características más relevantes para BoF\n",
        "selected_features = {}\n",
        "\n",
        "# Características CWT más importantes\n",
        "cwt_key_features = [\n",
        "    'cwt_energy_alpha',    # Energía en banda alpha (8-13 Hz)\n",
        "    'cwt_energy_beta',    # Energía en banda beta (13-30 Hz)\n",
        "    'cwt_dominant_freq',  # Frecuencia dominante\n",
        "    'cwt_spectral_entropy' # Entropía espectral\n",
        "]\n",
        "\n",
        "for feature_name in cwt_key_features:\n",
        "    if feature_name in cwt_features:\n",
        "        selected_features[feature_name] = cwt_features[feature_name]\n",
        "        print(f\"  - {feature_name}: {cwt_features[feature_name].shape}\")\n",
        "\n",
        "# Características DWT más importantes (primeros 3 niveles)\n",
        "dwt_key_features = [\n",
        "    'dwt_energy_level_0',  # Aproximación (baja frecuencia)\n",
        "    'dwt_energy_level_1',  # Primer detalle\n",
        "    'dwt_energy_level_2',  # Segundo detalle\n",
        "    'dwt_mean_level_0',    # Media de aproximación\n",
        "    'dwt_std_level_0'      # Desviación estándar de aproximación\n",
        "]\n",
        "\n",
        "for feature_name in dwt_key_features:\n",
        "    if feature_name in dwt_features:\n",
        "        selected_features[feature_name] = dwt_features[feature_name]\n",
        "        print(f\"  - {feature_name}: {dwt_features[feature_name].shape}\")\n",
        "\n",
        "print(f\"\\nTotal de características seleccionadas: {len(selected_features)}\")\n",
        "\n",
        "# Crear matriz de características seleccionadas\n",
        "bof_feature_names = []\n",
        "bof_feature_matrices = []\n",
        "\n",
        "for feature_name, feature_array in selected_features.items():\n",
        "    if len(feature_array.shape) == 2:  # (trials, channels)\n",
        "        for ch_idx, ch_name in enumerate(ch_names):\n",
        "            bof_feature_names.append(f\"{feature_name}_{ch_name}\")\n",
        "            bof_feature_matrices.append(feature_array[:, ch_idx])\n",
        "    elif len(feature_array.shape) == 3:  # (trials, channels, scales)\n",
        "        for ch_idx, ch_name in enumerate(ch_names):\n",
        "            for scale_idx in range(feature_array.shape[2]):\n",
        "                bof_feature_names.append(f\"{feature_name}_{ch_name}_scale{scale_idx}\")\n",
        "                bof_feature_matrices.append(feature_array[:, ch_idx, scale_idx])\n",
        "\n",
        "# Crear matriz final de características BoF\n",
        "X_bof = np.column_stack(bof_feature_matrices)\n",
        "\n",
        "print(f\"Matriz de características BoF:\")\n",
        "print(f\"  - Trials: {X_bof.shape[0]}\")\n",
        "print(f\"  - Características: {X_bof.shape[1]}\")\n",
        "print(f\"  - Primeras 10 características: {bof_feature_names[:10]}\")\n",
        "\n",
        "# Normalizar características BoF\n",
        "scaler_bof = StandardScaler()\n",
        "X_bof_normalized = scaler_bof.fit_transform(X_bof)\n",
        "\n",
        "print(f\"Características BoF normalizadas: {X_bof_normalized.shape}\")\n",
        "print(f\"Media después de normalización: {X_bof_normalized.mean():.6f}\")\n",
        "print(f\"Desviación estándar: {X_bof_normalized.std():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47oicjuJ8veH",
        "outputId": "bb268d37-415c-4005-b608-696f36bb9bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Guardando datos específicos para BoF...\n",
            "  - Características BoF: bof_data/X_bof_features.npy\n",
            "  - Nombres de características: bof_data/bof_feature_names.txt\n",
            "  - Normalizador BoF: bof_data/scaler_bof.pkl\n",
            "  - Metadatos BoF: bof_data/bof_metadata.json\n",
            "  - Configuración BoF: bof_data/bof_config.json\n",
            "\n",
            "Datos BoF guardados exitosamente en: /content/bof_data\n"
          ]
        }
      ],
      "source": [
        "# 3. Guardar datos específicos para BoF\n",
        "print(\"\\n3. Guardando datos específicos para BoF...\")\n",
        "\n",
        "# Guardar características BoF\n",
        "np.save(bof_data_dir / \"X_bof_features.npy\", X_bof_normalized)\n",
        "print(f\"  - Características BoF: {bof_data_dir / 'X_bof_features.npy'}\")\n",
        "\n",
        "# Guardar nombres de características BoF\n",
        "with open(bof_data_dir / \"bof_feature_names.txt\", 'w') as f:\n",
        "    for name in bof_feature_names:\n",
        "        f.write(f\"{name}\\n\")\n",
        "print(f\"  - Nombres de características: {bof_data_dir / 'bof_feature_names.txt'}\")\n",
        "\n",
        "# Guardar normalizador BoF\n",
        "import pickle\n",
        "with open(bof_data_dir / \"scaler_bof.pkl\", 'wb') as f:\n",
        "    pickle.dump(scaler_bof, f)\n",
        "print(f\"  - Normalizador BoF: {bof_data_dir / 'scaler_bof.pkl'}\")\n",
        "\n",
        "# Crear información de metadatos para BoF\n",
        "bof_metadata = {\n",
        "    'n_trials': X_bof_normalized.shape[0],\n",
        "    'n_features': X_bof_normalized.shape[1],\n",
        "    'n_channels': len(ch_names),\n",
        "    'n_subjects': len(subjects_info),\n",
        "    'class_distribution': {\n",
        "        'left_trials': int(np.sum(y_labels == 0)),\n",
        "        'right_trials': int(np.sum(y_labels == 1))\n",
        "    },\n",
        "    'feature_types': {\n",
        "        'cwt_features': len([f for f in bof_feature_names if f.startswith('cwt_')]),\n",
        "        'dwt_features': len([f for f in bof_feature_names if f.startswith('dwt_')])\n",
        "    },\n",
        "    'sampling_rate': sfreq,\n",
        "    'trial_duration_sec': T / sfreq,\n",
        "    'wavelet_config': {\n",
        "        'cwt_scales': len(CWT_SCALES),\n",
        "        'cwt_wavelet': CWT_WAVELET,\n",
        "        'dwt_levels': DWT_LEVELS,\n",
        "        'dwt_wavelet': DWT_WAVELET\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(bof_data_dir / \"bof_metadata.json\", 'w') as f:\n",
        "    json.dump(bof_metadata, f, indent=2)\n",
        "print(f\"  - Metadatos BoF: {bof_data_dir / 'bof_metadata.json'}\")\n",
        "\n",
        "# Crear archivo de configuración para BoF\n",
        "bof_config = {\n",
        "    'data_files': {\n",
        "        'features': 'X_bof_features.npy',\n",
        "        'labels': 'y_labels.npy',\n",
        "        'feature_names': 'bof_feature_names.txt',\n",
        "        'scaler': 'scaler_bof.pkl',\n",
        "        'metadata': 'bof_metadata.json'\n",
        "    },\n",
        "    'recommended_params': {\n",
        "        'n_clusters': [50, 100, 200],  # Número de clusters para BoF\n",
        "        'random_state': 42,\n",
        "        'test_size': 0.2,\n",
        "        'cv_folds': 5\n",
        "    },\n",
        "    'feature_info': {\n",
        "        'total_features': len(bof_feature_names),\n",
        "        'cwt_features': len([f for f in bof_feature_names if f.startswith('cwt_')]),\n",
        "        'dwt_features': len([f for f in bof_feature_names if f.startswith('dwt_')]),\n",
        "        'normalized': True,\n",
        "        'scaler_type': 'StandardScaler'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(bof_data_dir / \"bof_config.json\", 'w') as f:\n",
        "    json.dump(bof_config, f, indent=2)\n",
        "print(f\"  - Configuración BoF: {bof_data_dir / 'bof_config.json'}\")\n",
        "\n",
        "print(f\"\\nDatos BoF guardados exitosamente en: {bof_data_dir.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGlECWOS8veH"
      },
      "source": [
        "## Resumen del Análisis de Wavelets y Preparación BoF\n",
        "\n",
        "### Análisis Completados\n",
        "\n",
        "1. **Transformada Wavelet Continua (CWT)**\n",
        "   - Wavelet de Morlet con 50 escalas logarítmicas\n",
        "   - Análisis tiempo-frecuencia completo\n",
        "   - Extracción de energía por banda, frecuencia dominante y entropía espectral\n",
        "\n",
        "2. **Transformada Wavelet Discreta (DWT)**\n",
        "   - Wavelet Daubechies 4 con 6 niveles de descomposición\n",
        "   - Análisis multiresolución\n",
        "   - Extracción de estadísticas por nivel (energía, media, desviación estándar, máximo)\n",
        "\n",
        "3. **Preparación Específica para BoF**\n",
        "   - Selección de características más relevantes\n",
        "   - Normalización específica para BoF\n",
        "   - Etiquetas de clase (left/right)\n",
        "   - Metadatos completos y configuración\n",
        "\n",
        "### Archivos Generados para BoF\n",
        "\n",
        "Todos los archivos específicos para BoF se guardaron en el directorio `bof_data/`:\n",
        "\n",
        "- **`X_bof_features.npy`**: Características normalizadas específicas para BoF\n",
        "- **`y_labels.npy`**: Etiquetas de clase (0=left, 1=right)\n",
        "- **`bof_feature_names.txt`**: Nombres de características seleccionadas\n",
        "- **`scaler_bof.pkl`**: Normalizador entrenado para BoF\n",
        "- **`bof_metadata.json`**: Metadatos completos del dataset\n",
        "- **`bof_config.json`**: Configuración y parámetros recomendados\n",
        "- **`trial_to_subject.npy`**: Mapeo de trials a sujetos\n",
        "- **`trial_to_task.npy`**: Mapeo de trials a tareas\n",
        "\n",
        "### Características Seleccionadas para BoF\n",
        "\n",
        "**CWT Features:**\n",
        "- Energía en banda alpha (8-13 Hz)\n",
        "- Energía en banda beta (13-30 Hz)\n",
        "- Frecuencia dominante por canal\n",
        "- Entropía espectral por canal\n",
        "\n",
        "**DWT Features:**\n",
        "- Energía de aproximación (nivel 0)\n",
        "- Energía de detalles (niveles 1-2)\n",
        "- Media y desviación estándar de aproximación\n",
        "\n",
        "### Próximos Pasos para BoF\n",
        "\n",
        "Los datos están completamente preparados para implementar Bag of Features:\n",
        "\n",
        "1. **Cargar datos**: Usar archivos en `bof_data/`\n",
        "2. **Clustering**: Aplicar K-means con parámetros recomendados\n",
        "3. **Codificación**: Crear histogramas de características por trial\n",
        "4. **Clasificación**: Entrenar clasificadores SVM/Random Forest\n",
        "5. **Evaluación**: Validación cruzada y métricas de rendimiento\n",
        "\n",
        "### Variables Disponibles para BoF\n",
        "\n",
        "- `X_bof_normalized`: Características normalizadas para BoF\n",
        "- `y_labels`: Etiquetas de clase\n",
        "- `bof_feature_names`: Nombres de características\n",
        "- `scaler_bof`: Normalizador entrenado\n",
        "- `bof_metadata`: Metadatos del dataset\n",
        "- `bof_config`: Configuración recomendada\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}