{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Wavelets para Datos MI-EEG\n",
    "\n",
    "Este notebook implementa análisis avanzado de wavelets para extraer características temporales y espectrales de los datos EEG de imaginación motora.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. **Transformada Wavelet Continua (CWT)**: Análisis tiempo-frecuencia usando wavelet de Morlet\n",
    "2. **Transformada Wavelet Discreta (DWT)**: Descomposición multiresolución usando Daubechies 4\n",
    "3. **Extracción de características**: Energía por banda, frecuencia dominante, entropía espectral\n",
    "4. **Preparación para BoF**: Características optimizadas para Bag of Features\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "- Carga de datos EEG procesados\n",
    "- Análisis CWT con escalas logarítmicas\n",
    "- Análisis DWT con múltiples niveles\n",
    "- Extracción de características estadísticas\n",
    "- Visualizaciones tiempo-frecuencia\n",
    "- Guardado de características para BoF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in ./venv/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.13/site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in ./venv/lib/python3.13/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.7 in ./venv/lib/python3.13/site-packages (from mne) (3.10.6)\n",
      "Requirement already satisfied: numpy<3,>=1.25 in ./venv/lib/python3.13/site-packages (from mne) (2.3.3)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.13/site-packages (from mne) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in ./venv/lib/python3.13/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.11 in ./venv/lib/python3.13/site-packages (from mne) (1.16.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.13/site-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./venv/lib/python3.13/site-packages (from pooch>=1.5->mne) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.13/site-packages (from pooch>=1.5->mne) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->mne) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyWavelets in ./venv/lib/python3.13/site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25 in ./venv/lib/python3.13/site-packages (from PyWavelets) (2.3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/IPython/utils/_process_posix.py:125\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     child = \u001b[43mpexpect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-c\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[32m    126\u001b[39m flush = sys.stdout.flush\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:205\u001b[39m, in \u001b[36mspawn.__init__\u001b[39m\u001b[34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.use_poll = use_poll\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:303\u001b[39m, in \u001b[36mspawn._spawn\u001b[39m\u001b[34m(self, command, args, preexec_fn, dimensions)\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.args = [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a.encode(\u001b[38;5;28mself\u001b[39m.encoding)\n\u001b[32m    301\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args]\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28mself\u001b[39m.ptyproc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[38;5;28mself\u001b[39m.ptyproc.pid\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:315\u001b[39m, in \u001b[36mspawn._spawnpty\u001b[39m\u001b[34m(self, args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPtyProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/ptyprocess/ptyprocess.py:315\u001b[39m, in \u001b[36mPtyProcess.spawn\u001b[39m\u001b[34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[39m\n\u001b[32m    314\u001b[39m os.close(exec_err_pipe_write)\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m exec_err_data = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m os.close(exec_err_pipe_read)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall PyWavelets\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall scikit-learn\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstall matplotlib\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall pandas\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall numpy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2504\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2502\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2504\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2508\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/IPython/core/magics/packaging.py:105\u001b[39m, in \u001b[36mPackagingMagics.pip\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     python = shlex.quote(python)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/ipykernel/zmqshell.py:788\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/datos_BCI/venv/lib/python3.13/site-packages/IPython/utils/_process_posix.py:141\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    136\u001b[39m         out_size = \u001b[38;5;28mlen\u001b[39m(child.before)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mchild\u001b[49m.sendline(\u001b[38;5;28mchr\u001b[39m(\u001b[32m3\u001b[39m))\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Celda de instalación de dependencias\n",
    "# Ejecutar esta celda SOLO si necesitas instalar las librerías en un entorno nuevo\n",
    "\n",
    "%pip install mne\n",
    "%pip install PyWavelets\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scipy\n",
    "%pip install tqdm\n",
    "\n",
    "print(\"Instalación de dependencias completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas correctamente\n",
      "PyWavelets versión: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from mne.io import read_raw_eeglab\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configurar matplotlib\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"Librerías importadas correctamente\")\n",
    "print(f\"PyWavelets versión: {pywt.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos del EDA\n",
    "\n",
    "Este notebook continúa desde el análisis EDA previo. Cargamos los datos ya procesados para evitar duplicar el trabajo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos procesados del EDA...\n",
      "Datos cargados exitosamente:\n",
      "  - Datos concatenados: (880, 64, 1152) (trials, channels, time)\n",
      "  - Canales: 64\n",
      "  - Frecuencia de muestreo: 128.0 Hz\n",
      "  - Dimensiones: 880 trials, 64 canales, 1152 muestras\n",
      "  - Duración por trial: 9.0 segundos\n",
      "  - Sujetos procesados: 40\n",
      "  - Regiones identificadas: ['F', 'FC', 'C', 'CP', 'P', 'PO', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos procesados del EDA\n",
    "shared_data_dir = Path(\"shared_data\")\n",
    "\n",
    "if not shared_data_dir.exists():\n",
    "    print(\"Error: Directorio shared_data no encontrado.\")\n",
    "    print(\"Por favor ejecuta primero el notebook 01_EDA_Analysis.ipynb\")\n",
    "    raise FileNotFoundError(\"shared_data directory not found\")\n",
    "\n",
    "print(\"Cargando datos procesados del EDA...\")\n",
    "\n",
    "# Cargar datos principales\n",
    "X = np.load(shared_data_dir / \"X_data.npy\")  # Datos concatenados (trials, channels, time)\n",
    "ch_names = np.load(shared_data_dir / \"ch_names.npy\")  # Nombres de canales\n",
    "sfreq = float(np.load(shared_data_dir / \"sfreq.npy\")[0])  # Frecuencia de muestreo\n",
    "N, C, T = np.load(shared_data_dir / \"data_dimensions.npy\")  # Dimensiones\n",
    "\n",
    "# Cargar información de sujetos\n",
    "subjects_info_df = pd.read_csv(shared_data_dir / \"subjects_info.csv\")\n",
    "subjects_info = subjects_info_df.to_dict('records')\n",
    "\n",
    "# Cargar información de regiones\n",
    "with open(shared_data_dir / \"region_info.json\", 'r') as f:\n",
    "    region_info = json.load(f)\n",
    "region_indices = region_info['region_indices']\n",
    "REGION_PREFIXES = region_info['region_prefixes']\n",
    "\n",
    "# Cargar parámetros de configuración\n",
    "with open(shared_data_dir / \"config_params.json\", 'r') as f:\n",
    "    config_params = json.load(f)\n",
    "LOW_BAND = config_params['LOW_BAND']\n",
    "HIGH_BAND = config_params['HIGH_BAND']\n",
    "MU_BAND = tuple(config_params['MU_BAND'])\n",
    "BETA_BAND = tuple(config_params['BETA_BAND'])\n",
    "EXPECTED_TRIAL_SEC = config_params['EXPECTED_TRIAL_SEC']\n",
    "CANDIDATE_DIRS = config_params['CANDIDATE_DIRS']\n",
    "\n",
    "print(\"Datos cargados exitosamente:\")\n",
    "print(f\"  - Datos concatenados: {X.shape} (trials, channels, time)\")\n",
    "print(f\"  - Canales: {len(ch_names)}\")\n",
    "print(f\"  - Frecuencia de muestreo: {sfreq} Hz\")\n",
    "print(f\"  - Dimensiones: {N} trials, {C} canales, {T} muestras\")\n",
    "print(f\"  - Duración por trial: {T/sfreq:.1f} segundos\")\n",
    "print(f\"  - Sujetos procesados: {len(subjects_info)}\")\n",
    "print(f\"  - Regiones identificadas: {list(region_indices.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuración de directorios:\n",
      "  - Directorio de datos: /Users/manueljurado/Downloads/datos_BCI\n",
      "  - Directorio de salida EDA: /Users/manueljurado/Downloads/datos_BCI/reports\n",
      "  - Directorio de salida wavelets: /Users/manueljurado/Downloads/datos_BCI/wavelet_reports\n",
      "  - Directorio de datos compartidos: /Users/manueljurado/Downloads/datos_BCI/shared_data\n"
     ]
    }
   ],
   "source": [
    "# Configuración de directorios\n",
    "data_root = Path(\".\").resolve()  # Directorio actual\n",
    "output_dir = Path(\"reports\")\n",
    "wavelet_output_dir = Path(\"wavelet_reports\")\n",
    "wavelet_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nConfiguración de directorios:\")\n",
    "print(f\"  - Directorio de datos: {data_root}\")\n",
    "print(f\"  - Directorio de salida EDA: {output_dir.resolve()}\")\n",
    "print(f\"  - Directorio de salida wavelets: {wavelet_output_dir.resolve()}\")\n",
    "print(f\"  - Directorio de datos compartidos: {shared_data_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de Wavelets\n",
    "\n",
    "Definimos los parámetros para el análisis de wavelets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración de wavelets:\n",
      "Escalas CWT: 50 escalas desde 3.2 hasta 316.2\n",
      "Wavelet CWT: cmor5.0-1.0 con ancho 5.0\n",
      "Wavelet DWT: db4 con 6 niveles\n",
      "Bandas de frecuencia: ['delta', 'theta', 'alpha', 'beta', 'gamma']\n"
     ]
    }
   ],
   "source": [
    "# Configurar parámetros de wavelets\n",
    "CWT_SCALES = np.logspace(0.5, 2.5, 50)  # Escalas logarítmicas (1.6-316 Hz aprox)\n",
    "CWT_WAVELET = 'cmor5.0-1.0'  # Complex Morlet wavelet para análisis tiempo-frecuencia (ancho 5.0, frecuencia central 1.0)\n",
    "CWT_WIDTH = 5.0  # Ancho de la wavelet de Morlet\n",
    "\n",
    "# Parámetros DWT\n",
    "DWT_WAVELET = 'db4'  # Wavelet Daubechies 4\n",
    "DWT_LEVELS = 6  # Niveles de descomposición\n",
    "\n",
    "# Bandas de frecuencia de interés para wavelets\n",
    "FREQ_BANDS = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'gamma': (30, 100)\n",
    "}\n",
    "\n",
    "print(\"Configuración de wavelets:\")\n",
    "print(f\"Escalas CWT: {len(CWT_SCALES)} escalas desde {CWT_SCALES[0]:.1f} hasta {CWT_SCALES[-1]:.1f}\")\n",
    "print(f\"Wavelet CWT: {CWT_WAVELET} con ancho {CWT_WIDTH}\")\n",
    "print(f\"Wavelet DWT: {DWT_WAVELET} con {DWT_LEVELS} niveles\")\n",
    "print(f\"Bandas de frecuencia: {list(FREQ_BANDS.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares para Wavelets\n",
    "\n",
    "Definimos las funciones específicas para el análisis de wavelets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones auxiliares para wavelets definidas\n"
     ]
    }
   ],
   "source": [
    "def scales_to_frequencies(scales: np.ndarray, wavelet: str, sampling_rate: float) -> np.ndarray:\n",
    "    \"\"\"Convierte escalas wavelet a frecuencias.\"\"\"\n",
    "    if wavelet.startswith('cmor'):\n",
    "        # Para Complex Morlet: freq = (width * sampling_rate) / (2 * pi * scale)\n",
    "        # Extraer ancho de banda del nombre de la wavelet (formato: cmorB-C)\n",
    "        if '-' in wavelet:\n",
    "            bandwidth = float(wavelet.split('-')[0].replace('cmor', ''))\n",
    "        else:\n",
    "            bandwidth = CWT_WIDTH\n",
    "        frequencies = (bandwidth * sampling_rate) / (2 * np.pi * scales)\n",
    "    else:\n",
    "        # Para otras wavelets, aproximación general\n",
    "        frequencies = sampling_rate / (2 * scales)\n",
    "    return frequencies\n",
    "\n",
    "def compute_cwt_coefficients(data: np.ndarray, sfreq: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calcula coeficientes CWT para todos los canales y trials.\n",
    "    \n",
    "    Args:\n",
    "        data: Array de forma (trials, channels, time)\n",
    "        sfreq: Frecuencia de muestreo\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (coeficientes_cwt, frecuencias)\n",
    "    \"\"\"\n",
    "    n_trials, n_channels, n_times = data.shape\n",
    "    \n",
    "    # Calcular frecuencias correspondientes a las escalas\n",
    "    frequencies = scales_to_frequencies(CWT_SCALES, CWT_WAVELET, sfreq)\n",
    "    \n",
    "    # Inicializar array para coeficientes CWT\n",
    "    cwt_coeffs = np.zeros((n_trials, n_channels, len(CWT_SCALES), n_times), dtype=complex)\n",
    "    \n",
    "    print(\"Calculando coeficientes CWT...\")\n",
    "    for trial_idx in tqdm(range(n_trials), desc=\"Trials\"):\n",
    "        for ch_idx in range(n_channels):\n",
    "            signal = data[trial_idx, ch_idx, :]\n",
    "            \n",
    "            # Calcular CWT\n",
    "            coefficients, _ = pywt.cwt(signal, CWT_SCALES, CWT_WAVELET, \n",
    "                                     sampling_period=1.0/sfreq)\n",
    "            cwt_coeffs[trial_idx, ch_idx, :, :] = coefficients\n",
    "    \n",
    "    return cwt_coeffs, frequencies\n",
    "\n",
    "print(\"Funciones auxiliares para wavelets definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis CWT (Transformada Wavelet Continua)\n",
    "\n",
    "Calculamos los coeficientes CWT para análisis tiempo-frecuencia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando análisis CWT...\n",
      "Datos de entrada: (880, 64, 1152) (trials, channels, time)\n",
      "Frecuencia de muestreo: 128.0 Hz\n",
      "Calculando coeficientes CWT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trials: 100%|██████████| 880/880 [45:34<00:00,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CWT completado:\n",
      "Coeficientes CWT: (880, 64, 50, 1152)\n",
      "Frecuencias CWT: 50 puntos\n",
      "Rango de frecuencias: 32.2 - 0.3 Hz\n",
      "\n",
      "Bandas de frecuencia CWT:\n",
      "  - delta (1-4 Hz): 14 escalas\n",
      "  - theta (4-8 Hz): 8 escalas\n",
      "  - alpha (8-13 Hz): 5 escalas\n",
      "  - beta (13-30 Hz): 9 escalas\n",
      "  - gamma (30-100 Hz): 1 escalas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular coeficientes CWT usando los datos concatenados\n",
    "print(\"Iniciando análisis CWT...\")\n",
    "print(f\"Datos de entrada: {X.shape} (trials, channels, time)\")\n",
    "print(f\"Frecuencia de muestreo: {sfreq} Hz\")\n",
    "\n",
    "# Calcular CWT\n",
    "cwt_coeffs, cwt_frequencies = compute_cwt_coefficients(X, sfreq)\n",
    "\n",
    "print(f\"\\nCWT completado:\")\n",
    "print(f\"Coeficientes CWT: {cwt_coeffs.shape}\")\n",
    "print(f\"Frecuencias CWT: {len(cwt_frequencies)} puntos\")\n",
    "print(f\"Rango de frecuencias: {cwt_frequencies[0]:.1f} - {cwt_frequencies[-1]:.1f} Hz\")\n",
    "\n",
    "# Mostrar información sobre las bandas de frecuencia\n",
    "print(f\"\\nBandas de frecuencia CWT:\")\n",
    "for band_name, (fmin, fmax) in FREQ_BANDS.items():\n",
    "    mask = (cwt_frequencies >= fmin) & (cwt_frequencies <= fmax)\n",
    "    n_scales = mask.sum()\n",
    "    if n_scales > 0:\n",
    "        print(f\"  - {band_name} ({fmin}-{fmax} Hz): {n_scales} escalas\")\n",
    "    else:\n",
    "        print(f\"  - {band_name} ({fmin}-{fmax} Hz): fuera del rango\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de Características CWT\n",
    "\n",
    "Extraemos características estadísticas de los coeficientes CWT para BoF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características CWT...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def extract_cwt_features(cwt_coeffs: np.ndarray, frequencies: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extrae características de los coeficientes CWT.\n",
    "    \n",
    "    Args:\n",
    "        cwt_coeffs: Coeficientes CWT de forma (trials, channels, scales, time)\n",
    "        frequencies: Array de frecuencias correspondientes\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con características extraídas\n",
    "    \"\"\"\n",
    "    n_trials, n_channels, n_scales, n_times = cwt_coeffs.shape\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    print(\"Extrayendo características CWT...\")\n",
    "    \n",
    "    # 1. Energía por banda de frecuencia\n",
    "    for band_name, (fmin, fmax) in FREQ_BANDS.items():\n",
    "        # Encontrar índices de escalas correspondientes a esta banda\n",
    "        band_mask = (frequencies >= fmin) & (frequencies <= fmax)\n",
    "        if not band_mask.any():\n",
    "            continue\n",
    "            \n",
    "        # Energía promedio por trial y canal\n",
    "        band_coeffs = cwt_coeffs[:, :, band_mask, :]\n",
    "        energy = np.mean(np.abs(band_coeffs)**2, axis=(2, 3))  # (trials, channels)\n",
    "        features[f'cwt_energy_{band_name}'] = energy\n",
    "        print(f\"  - Energía {band_name}: {energy.shape}\")\n",
    "    \n",
    "    # 2. Potencia máxima por escala\n",
    "    max_power_per_scale = np.max(np.abs(cwt_coeffs)**2, axis=3)  # (trials, channels, scales)\n",
    "    features['cwt_max_power'] = max_power_per_scale\n",
    "    print(f\"  - Potencia máxima por escala: {max_power_per_scale.shape}\")\n",
    "    \n",
    "    # 3. Frecuencia dominante por trial y canal\n",
    "    power_spectrum = np.abs(cwt_coeffs)**2\n",
    "    dominant_freq_idx = np.argmax(np.mean(power_spectrum, axis=3), axis=2)  # (trials, channels)\n",
    "    dominant_frequencies = frequencies[dominant_freq_idx]\n",
    "    features['cwt_dominant_freq'] = dominant_frequencies\n",
    "    print(f\"  - Frecuencia dominante: {dominant_frequencies.shape}\")\n",
    "    \n",
    "    # 4. Entropía espectral\n",
    "    spectral_entropy = np.zeros((n_trials, n_channels))\n",
    "    for trial_idx in range(n_trials):\n",
    "        for ch_idx in range(n_channels):\n",
    "            power = np.mean(np.abs(cwt_coeffs[trial_idx, ch_idx, :, :])**2, axis=1)\n",
    "            power_norm = power / np.sum(power)\n",
    "            spectral_entropy[trial_idx, ch_idx] = -np.sum(power_norm * np.log(power_norm + 1e-10))\n",
    "    features['cwt_spectral_entropy'] = spectral_entropy\n",
    "    print(f\"  - Entropía espectral: {spectral_entropy.shape}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extraer características CWT\n",
    "cwt_features = extract_cwt_features(cwt_coeffs, cwt_frequencies)\n",
    "\n",
    "print(f\"\\nCaracterísticas CWT extraídas:\")\n",
    "for feature_name, feature_array in cwt_features.items():\n",
    "    print(f\"  - {feature_name}: {feature_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis DWT (Transformada Wavelet Discreta)\n",
    "\n",
    "Implementamos análisis DWT para descomposición multiresolución:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dwt_coefficients(data: np.ndarray, wavelet: str, levels: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calcula coeficientes DWT para todos los canales y trials.\n",
    "    \n",
    "    Args:\n",
    "        data: Array de forma (trials, channels, time)\n",
    "        wavelet: Tipo de wavelet (ej. 'db4')\n",
    "        levels: Número de niveles de descomposición\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con coeficientes DWT por nivel\n",
    "    \"\"\"\n",
    "    n_trials, n_channels, n_times = data.shape\n",
    "    \n",
    "    # Inicializar diccionario para coeficientes\n",
    "    dwt_coeffs = {}\n",
    "    \n",
    "    print(\"Calculando coeficientes DWT...\")\n",
    "    for trial_idx in tqdm(range(n_trials), desc=\"Trials\"):\n",
    "        for ch_idx in range(n_channels):\n",
    "            signal = data[trial_idx, ch_idx, :]\n",
    "            \n",
    "            # Calcular DWT\n",
    "            coeffs = pywt.wavedec(signal, wavelet, level=levels)\n",
    "            \n",
    "            # Guardar coeficientes por nivel\n",
    "            for level, coeff in enumerate(coeffs):\n",
    "                key = f'level_{level}'\n",
    "                if key not in dwt_coeffs:\n",
    "                    dwt_coeffs[key] = np.zeros((n_trials, n_channels, len(coeff)))\n",
    "                dwt_coeffs[key][trial_idx, ch_idx, :] = coeff\n",
    "    \n",
    "    return dwt_coeffs\n",
    "\n",
    "# Calcular coeficientes DWT\n",
    "print(\"Iniciando análisis DWT...\")\n",
    "print(f\"Datos de entrada: {X.shape} (trials, channels, time)\")\n",
    "print(f\"Wavelet: {DWT_WAVELET}, Niveles: {DWT_LEVELS}\")\n",
    "\n",
    "dwt_coeffs = compute_dwt_coefficients(X, DWT_WAVELET, DWT_LEVELS)\n",
    "\n",
    "print(f\"\\nDWT completado:\")\n",
    "print(f\"Coeficientes DWT por nivel:\")\n",
    "for level_key, coeff_array in dwt_coeffs.items():\n",
    "    print(f\"  - {level_key}: {coeff_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de Características DWT\n",
    "\n",
    "Extraemos características estadísticas de los coeficientes DWT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dwt_features(dwt_coeffs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extrae características de los coeficientes DWT.\n",
    "    \n",
    "    Args:\n",
    "        dwt_coeffs: Diccionario con coeficientes DWT por nivel\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con características extraídas\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    print(\"Extrayendo características DWT...\")\n",
    "    \n",
    "    # 1. Energía por nivel de descomposición\n",
    "    for level_key, coeff_array in dwt_coeffs.items():\n",
    "        # Energía promedio por trial y canal\n",
    "        energy = np.mean(coeff_array**2, axis=2)  # (trials, channels)\n",
    "        features[f'dwt_energy_{level_key}'] = energy\n",
    "        print(f\"  - Energía {level_key}: {energy.shape}\")\n",
    "    \n",
    "    # 2. Estadísticas por nivel\n",
    "    for level_key, coeff_array in dwt_coeffs.items():\n",
    "        # Media por trial y canal\n",
    "        mean_coeffs = np.mean(coeff_array, axis=2)  # (trials, channels)\n",
    "        features[f'dwt_mean_{level_key}'] = mean_coeffs\n",
    "        \n",
    "        # Desviación estándar por trial y canal\n",
    "        std_coeffs = np.std(coeff_array, axis=2)  # (trials, channels)\n",
    "        features[f'dwt_std_{level_key}'] = std_coeffs\n",
    "        \n",
    "        # Máximo absoluto por trial y canal\n",
    "        max_coeffs = np.max(np.abs(coeff_array), axis=2)  # (trials, channels)\n",
    "        features[f'dwt_max_{level_key}'] = max_coeffs\n",
    "        \n",
    "        print(f\"  - Estadísticas {level_key}: media, std, max\")\n",
    "    \n",
    "    # 3. Relación de energía entre niveles (aproximación de bandas de frecuencia)\n",
    "    if 'level_0' in dwt_coeffs and 'level_1' in dwt_coeffs:\n",
    "        # Relación entre aproximación y detalle\n",
    "        energy_level_0 = np.mean(dwt_coeffs['level_0']**2, axis=2)\n",
    "        energy_level_1 = np.mean(dwt_coeffs['level_1']**2, axis=2)\n",
    "        energy_ratio = energy_level_0 / (energy_level_1 + 1e-10)\n",
    "        features['dwt_energy_ratio_level0_level1'] = energy_ratio\n",
    "        print(f\"  - Relación de energía level0/level1: {energy_ratio.shape}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extraer características DWT\n",
    "dwt_features = extract_dwt_features(dwt_coeffs)\n",
    "\n",
    "print(f\"\\nCaracterísticas DWT extraídas:\")\n",
    "for feature_name, feature_array in dwt_features.items():\n",
    "    print(f\"  - {feature_name}: {feature_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de Características para BoF\n",
    "\n",
    "Combinamos todas las características extraídas y las preparamos para Bag of Features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todas las características\n",
    "print(\"Combinando características para BoF...\")\n",
    "\n",
    "# Combinar características CWT y DWT\n",
    "all_features = {}\n",
    "all_features.update(cwt_features)\n",
    "all_features.update(dwt_features)\n",
    "\n",
    "print(f\"Total de características extraídas: {len(all_features)}\")\n",
    "\n",
    "# Crear matriz de características para BoF\n",
    "feature_names = []\n",
    "feature_matrices = []\n",
    "\n",
    "for feature_name, feature_array in all_features.items():\n",
    "    # Reshape para tener una matriz 2D (samples, features)\n",
    "    if len(feature_array.shape) == 2:  # (trials, channels)\n",
    "        # Cada canal es una característica\n",
    "        for ch_idx, ch_name in enumerate(ch_names):\n",
    "            feature_names.append(f\"{feature_name}_{ch_name}\")\n",
    "            feature_matrices.append(feature_array[:, ch_idx])\n",
    "    elif len(feature_array.shape) == 3:  # (trials, channels, scales/levels)\n",
    "        # Cada combinación canal-escala es una característica\n",
    "        for ch_idx, ch_name in enumerate(ch_names):\n",
    "            for scale_idx in range(feature_array.shape[2]):\n",
    "                feature_names.append(f\"{feature_name}_{ch_name}_scale{scale_idx}\")\n",
    "                feature_matrices.append(feature_array[:, ch_idx, scale_idx])\n",
    "\n",
    "# Crear matriz final de características\n",
    "X_features = np.column_stack(feature_matrices)  # (trials, total_features)\n",
    "\n",
    "print(f\"Matriz de características final:\")\n",
    "print(f\"  - Trials: {X_features.shape[0]}\")\n",
    "print(f\"  - Características totales: {X_features.shape[1]}\")\n",
    "print(f\"  - Primeras 10 características: {feature_names[:10]}\")\n",
    "\n",
    "# Normalizar características\n",
    "scaler = StandardScaler()\n",
    "X_features_normalized = scaler.fit_transform(X_features)\n",
    "\n",
    "print(f\"Características normalizadas: {X_features_normalized.shape}\")\n",
    "print(f\"Media después de normalización: {X_features_normalized.mean():.6f}\")\n",
    "print(f\"Desviación estándar después de normalización: {X_features_normalized.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de Archivos para BoF\n",
    "\n",
    "Guardamos las características y metadatos necesarios para implementar Bag of Features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar características normalizadas para BoF\n",
    "features_file = wavelet_output_dir / \"wavelet_features.npy\"\n",
    "np.save(features_file, X_features_normalized)\n",
    "print(f\"Características guardadas: {features_file.resolve()}\")\n",
    "\n",
    "# Guardar nombres de características\n",
    "feature_names_file = wavelet_output_dir / \"feature_names.txt\"\n",
    "with open(feature_names_file, 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Nombres de características guardados: {feature_names_file.resolve()}\")\n",
    "\n",
    "# Guardar información de canales\n",
    "channel_info = pd.DataFrame({\n",
    "    'channel_index': range(len(ch_names)),\n",
    "    'channel_name': ch_names,\n",
    "    'region': ['unknown'] * len(ch_names)  # Se puede mejorar con mapeo de regiones\n",
    "})\n",
    "\n",
    "# Mapear regiones basado en prefijos\n",
    "for idx, ch_name in enumerate(ch_names):\n",
    "    ch_upper = ch_name.upper()\n",
    "    for region, prefixes in REGION_PREFIXES.items():\n",
    "        if any(ch_upper.startswith(p) for p in prefixes):\n",
    "            channel_info.loc[idx, 'region'] = region\n",
    "            break\n",
    "\n",
    "channel_info_file = wavelet_output_dir / \"channel_info.csv\"\n",
    "channel_info.to_csv(channel_info_file, index=False)\n",
    "print(f\"Información de canales guardada: {channel_info_file.resolve()}\")\n",
    "\n",
    "# Guardar información de sujetos y tareas\n",
    "subjects_df = pd.DataFrame(subjects_info)\n",
    "subjects_file = wavelet_output_dir / \"subjects_info.csv\"\n",
    "subjects_df.to_csv(subjects_file, index=False)\n",
    "print(f\"Información de sujetos guardada: {subjects_file.resolve()}\")\n",
    "\n",
    "# Guardar parámetros de configuración\n",
    "config_info = {\n",
    "    'cwt_scales': CWT_SCALES.tolist(),\n",
    "    'cwt_wavelet': CWT_WAVELET,\n",
    "    'cwt_width': CWT_WIDTH,\n",
    "    'dwt_wavelet': DWT_WAVELET,\n",
    "    'dwt_levels': DWT_LEVELS,\n",
    "    'freq_bands': FREQ_BANDS,\n",
    "    'sampling_rate': sfreq,\n",
    "    'n_trials': N,\n",
    "    'n_channels': C,\n",
    "    'n_timepoints': T,\n",
    "    'feature_dimensions': X_features_normalized.shape[1]\n",
    "}\n",
    "\n",
    "config_file = wavelet_output_dir / \"wavelet_config.json\"\n",
    "import json\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "print(f\"Configuración guardada: {config_file.resolve()}\")\n",
    "\n",
    "print(f\"\\nResumen de archivos generados:\")\n",
    "print(f\"  - {features_file.name}: Características normalizadas ({X_features_normalized.shape})\")\n",
    "print(f\"  - {feature_names_file.name}: Nombres de características ({len(feature_names)} características)\")\n",
    "print(f\"  - {channel_info_file.name}: Información de canales ({len(ch_names)} canales)\")\n",
    "print(f\"  - {subjects_file.name}: Información de sujetos ({len(subjects_info)} archivos)\")\n",
    "print(f\"  - {config_file.name}: Parámetros de configuración\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de Datos para Bag of Features (BoF)\n",
    "\n",
    "Esta sección prepara específicamente los datos que necesita el algoritmo Bag of Features para clasificación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio específico para datos de BoF\n",
    "bof_data_dir = Path(\"bof_data\")\n",
    "bof_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Preparando datos específicos para Bag of Features...\")\n",
    "print(f\"Directorio BoF: {bof_data_dir.resolve()}\")\n",
    "\n",
    "# 1. Crear etiquetas de clase basadas en la tarea (left/right)\n",
    "print(\"\\n1. Creando etiquetas de clase...\")\n",
    "\n",
    "# Crear array de etiquetas basado en subjects_info\n",
    "y_labels = []\n",
    "trial_to_subject = []  # Mapeo de trial a sujeto\n",
    "trial_to_task = []     # Mapeo de trial a tarea\n",
    "\n",
    "trial_idx = 0\n",
    "for subject_info in subjects_info:\n",
    "    n_trials = subject_info['n_trials']\n",
    "    task = subject_info['task']\n",
    "    subject = subject_info['subject']\n",
    "    \n",
    "    # Etiquetas: 0 = left, 1 = right\n",
    "    label = 0 if task == 'left' else 1\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        y_labels.append(label)\n",
    "        trial_to_subject.append(subject)\n",
    "        trial_to_task.append(task)\n",
    "        trial_idx += 1\n",
    "\n",
    "y_labels = np.array(y_labels)\n",
    "print(f\"  - Etiquetas creadas: {y_labels.shape}\")\n",
    "print(f\"  - Clase 0 (left): {np.sum(y_labels == 0)} trials\")\n",
    "print(f\"  - Clase 1 (right): {np.sum(y_labels == 1)} trials\")\n",
    "\n",
    "# Guardar etiquetas\n",
    "np.save(bof_data_dir / \"y_labels.npy\", y_labels)\n",
    "np.save(bof_data_dir / \"trial_to_subject.npy\", np.array(trial_to_subject))\n",
    "np.save(bof_data_dir / \"trial_to_task.npy\", np.array(trial_to_task))\n",
    "\n",
    "print(f\"  - Etiquetas guardadas en: {bof_data_dir / 'y_labels.npy'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preparar características específicas para BoF\n",
    "print(\"\\n2. Preparando características para BoF...\")\n",
    "\n",
    "# Seleccionar características más relevantes para BoF\n",
    "selected_features = {}\n",
    "\n",
    "# Características CWT más importantes\n",
    "cwt_key_features = [\n",
    "    'cwt_energy_alpha',    # Energía en banda alpha (8-13 Hz)\n",
    "    'cwt_energy_beta',    # Energía en banda beta (13-30 Hz)\n",
    "    'cwt_dominant_freq',  # Frecuencia dominante\n",
    "    'cwt_spectral_entropy' # Entropía espectral\n",
    "]\n",
    "\n",
    "for feature_name in cwt_key_features:\n",
    "    if feature_name in cwt_features:\n",
    "        selected_features[feature_name] = cwt_features[feature_name]\n",
    "        print(f\"  - {feature_name}: {cwt_features[feature_name].shape}\")\n",
    "\n",
    "# Características DWT más importantes (primeros 3 niveles)\n",
    "dwt_key_features = [\n",
    "    'dwt_energy_level_0',  # Aproximación (baja frecuencia)\n",
    "    'dwt_energy_level_1',  # Primer detalle\n",
    "    'dwt_energy_level_2',  # Segundo detalle\n",
    "    'dwt_mean_level_0',    # Media de aproximación\n",
    "    'dwt_std_level_0'      # Desviación estándar de aproximación\n",
    "]\n",
    "\n",
    "for feature_name in dwt_key_features:\n",
    "    if feature_name in dwt_features:\n",
    "        selected_features[feature_name] = dwt_features[feature_name]\n",
    "        print(f\"  - {feature_name}: {dwt_features[feature_name].shape}\")\n",
    "\n",
    "print(f\"\\nTotal de características seleccionadas: {len(selected_features)}\")\n",
    "\n",
    "# Crear matriz de características seleccionadas\n",
    "bof_feature_names = []\n",
    "bof_feature_matrices = []\n",
    "\n",
    "for feature_name, feature_array in selected_features.items():\n",
    "    if len(feature_array.shape) == 2:  # (trials, channels)\n",
    "        for ch_idx, ch_name in enumerate(ch_names):\n",
    "            bof_feature_names.append(f\"{feature_name}_{ch_name}\")\n",
    "            bof_feature_matrices.append(feature_array[:, ch_idx])\n",
    "    elif len(feature_array.shape) == 3:  # (trials, channels, scales)\n",
    "        for ch_idx, ch_name in enumerate(ch_names):\n",
    "            for scale_idx in range(feature_array.shape[2]):\n",
    "                bof_feature_names.append(f\"{feature_name}_{ch_name}_scale{scale_idx}\")\n",
    "                bof_feature_matrices.append(feature_array[:, ch_idx, scale_idx])\n",
    "\n",
    "# Crear matriz final de características BoF\n",
    "X_bof = np.column_stack(bof_feature_matrices)\n",
    "\n",
    "print(f\"Matriz de características BoF:\")\n",
    "print(f\"  - Trials: {X_bof.shape[0]}\")\n",
    "print(f\"  - Características: {X_bof.shape[1]}\")\n",
    "print(f\"  - Primeras 10 características: {bof_feature_names[:10]}\")\n",
    "\n",
    "# Normalizar características BoF\n",
    "scaler_bof = StandardScaler()\n",
    "X_bof_normalized = scaler_bof.fit_transform(X_bof)\n",
    "\n",
    "print(f\"Características BoF normalizadas: {X_bof_normalized.shape}\")\n",
    "print(f\"Media después de normalización: {X_bof_normalized.mean():.6f}\")\n",
    "print(f\"Desviación estándar: {X_bof_normalized.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Guardar datos específicos para BoF\n",
    "print(\"\\n3. Guardando datos específicos para BoF...\")\n",
    "\n",
    "# Guardar características BoF\n",
    "np.save(bof_data_dir / \"X_bof_features.npy\", X_bof_normalized)\n",
    "print(f\"  - Características BoF: {bof_data_dir / 'X_bof_features.npy'}\")\n",
    "\n",
    "# Guardar nombres de características BoF\n",
    "with open(bof_data_dir / \"bof_feature_names.txt\", 'w') as f:\n",
    "    for name in bof_feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"  - Nombres de características: {bof_data_dir / 'bof_feature_names.txt'}\")\n",
    "\n",
    "# Guardar normalizador BoF\n",
    "import pickle\n",
    "with open(bof_data_dir / \"scaler_bof.pkl\", 'wb') as f:\n",
    "    pickle.dump(scaler_bof, f)\n",
    "print(f\"  - Normalizador BoF: {bof_data_dir / 'scaler_bof.pkl'}\")\n",
    "\n",
    "# Crear información de metadatos para BoF\n",
    "bof_metadata = {\n",
    "    'n_trials': X_bof_normalized.shape[0],\n",
    "    'n_features': X_bof_normalized.shape[1],\n",
    "    'n_channels': len(ch_names),\n",
    "    'n_subjects': len(subjects_info),\n",
    "    'class_distribution': {\n",
    "        'left_trials': int(np.sum(y_labels == 0)),\n",
    "        'right_trials': int(np.sum(y_labels == 1))\n",
    "    },\n",
    "    'feature_types': {\n",
    "        'cwt_features': len([f for f in bof_feature_names if f.startswith('cwt_')]),\n",
    "        'dwt_features': len([f for f in bof_feature_names if f.startswith('dwt_')])\n",
    "    },\n",
    "    'sampling_rate': sfreq,\n",
    "    'trial_duration_sec': T / sfreq,\n",
    "    'wavelet_config': {\n",
    "        'cwt_scales': len(CWT_SCALES),\n",
    "        'cwt_wavelet': CWT_WAVELET,\n",
    "        'dwt_levels': DWT_LEVELS,\n",
    "        'dwt_wavelet': DWT_WAVELET\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(bof_data_dir / \"bof_metadata.json\", 'w') as f:\n",
    "    json.dump(bof_metadata, f, indent=2)\n",
    "print(f\"  - Metadatos BoF: {bof_data_dir / 'bof_metadata.json'}\")\n",
    "\n",
    "# Crear archivo de configuración para BoF\n",
    "bof_config = {\n",
    "    'data_files': {\n",
    "        'features': 'X_bof_features.npy',\n",
    "        'labels': 'y_labels.npy',\n",
    "        'feature_names': 'bof_feature_names.txt',\n",
    "        'scaler': 'scaler_bof.pkl',\n",
    "        'metadata': 'bof_metadata.json'\n",
    "    },\n",
    "    'recommended_params': {\n",
    "        'n_clusters': [50, 100, 200],  # Número de clusters para BoF\n",
    "        'random_state': 42,\n",
    "        'test_size': 0.2,\n",
    "        'cv_folds': 5\n",
    "    },\n",
    "    'feature_info': {\n",
    "        'total_features': len(bof_feature_names),\n",
    "        'cwt_features': len([f for f in bof_feature_names if f.startswith('cwt_')]),\n",
    "        'dwt_features': len([f for f in bof_feature_names if f.startswith('dwt_')]),\n",
    "        'normalized': True,\n",
    "        'scaler_type': 'StandardScaler'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(bof_data_dir / \"bof_config.json\", 'w') as f:\n",
    "    json.dump(bof_config, f, indent=2)\n",
    "print(f\"  - Configuración BoF: {bof_data_dir / 'bof_config.json'}\")\n",
    "\n",
    "print(f\"\\nDatos BoF guardados exitosamente en: {bof_data_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Análisis de Wavelets y Preparación BoF\n",
    "\n",
    "### Análisis Completados\n",
    "\n",
    "1. **Transformada Wavelet Continua (CWT)**\n",
    "   - Wavelet de Morlet con 50 escalas logarítmicas\n",
    "   - Análisis tiempo-frecuencia completo\n",
    "   - Extracción de energía por banda, frecuencia dominante y entropía espectral\n",
    "\n",
    "2. **Transformada Wavelet Discreta (DWT)**\n",
    "   - Wavelet Daubechies 4 con 6 niveles de descomposición\n",
    "   - Análisis multiresolución\n",
    "   - Extracción de estadísticas por nivel (energía, media, desviación estándar, máximo)\n",
    "\n",
    "3. **Preparación Específica para BoF**\n",
    "   - Selección de características más relevantes\n",
    "   - Normalización específica para BoF\n",
    "   - Etiquetas de clase (left/right)\n",
    "   - Metadatos completos y configuración\n",
    "\n",
    "### Archivos Generados para BoF\n",
    "\n",
    "Todos los archivos específicos para BoF se guardaron en el directorio `bof_data/`:\n",
    "\n",
    "- **`X_bof_features.npy`**: Características normalizadas específicas para BoF\n",
    "- **`y_labels.npy`**: Etiquetas de clase (0=left, 1=right)\n",
    "- **`bof_feature_names.txt`**: Nombres de características seleccionadas\n",
    "- **`scaler_bof.pkl`**: Normalizador entrenado para BoF\n",
    "- **`bof_metadata.json`**: Metadatos completos del dataset\n",
    "- **`bof_config.json`**: Configuración y parámetros recomendados\n",
    "- **`trial_to_subject.npy`**: Mapeo de trials a sujetos\n",
    "- **`trial_to_task.npy`**: Mapeo de trials a tareas\n",
    "\n",
    "### Características Seleccionadas para BoF\n",
    "\n",
    "**CWT Features:**\n",
    "- Energía en banda alpha (8-13 Hz)\n",
    "- Energía en banda beta (13-30 Hz)\n",
    "- Frecuencia dominante por canal\n",
    "- Entropía espectral por canal\n",
    "\n",
    "**DWT Features:**\n",
    "- Energía de aproximación (nivel 0)\n",
    "- Energía de detalles (niveles 1-2)\n",
    "- Media y desviación estándar de aproximación\n",
    "\n",
    "### Próximos Pasos para BoF\n",
    "\n",
    "Los datos están completamente preparados para implementar Bag of Features:\n",
    "\n",
    "1. **Cargar datos**: Usar archivos en `bof_data/`\n",
    "2. **Clustering**: Aplicar K-means con parámetros recomendados\n",
    "3. **Codificación**: Crear histogramas de características por trial\n",
    "4. **Clasificación**: Entrenar clasificadores SVM/Random Forest\n",
    "5. **Evaluación**: Validación cruzada y métricas de rendimiento\n",
    "\n",
    "### Variables Disponibles para BoF\n",
    "\n",
    "- `X_bof_normalized`: Características normalizadas para BoF\n",
    "- `y_labels`: Etiquetas de clase\n",
    "- `bof_feature_names`: Nombres de características\n",
    "- `scaler_bof`: Normalizador entrenado\n",
    "- `bof_metadata`: Metadatos del dataset\n",
    "- `bof_config`: Configuración recomendada\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
